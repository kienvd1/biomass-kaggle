{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSIRO Image2Biomass – DINOv3 Direct Model Inference\n",
        "\n",
        "This notebook runs inference using DINOv3 Direct models trained with `dinov3_train.py`.\n",
        "\n",
        "**Model:**\n",
        "- `DINOv3Direct`: Predicts Total, Green, GDM directly; derives Dead, Clover\n",
        "- Components always sum to Total (mathematically guaranteed)\n",
        "\n",
        "**Features:**\n",
        "- Multi-fold ensemble (5-fold CV)\n",
        "- Test-Time Augmentation (TTA)\n",
        "- MPS/CUDA/CPU support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: mps\n",
            "============================================================\n",
            "DINOv3 Direct Model Inference\n",
            "============================================================\n",
            "Device: mps\n",
            "Model dir: /Users/kienvu/Desktop/kaggle/biomass/outputs/dinov3_20251218_174413\n",
            "TTA: True\n",
            "Loaded config from results.json\n",
            "Model: DINOv3Direct\n",
            "Backbone: vit_base_patch16_dinov3\n",
            "Grid: (2, 2)\n",
            "Image size: 576\n",
            "Optional heads: train_dead=False, train_clover=True\n",
            "Selecting top 3 folds by aR²:\n",
            "  Fold 2: aR²=0.7024\n",
            "  Fold 4: aR²=0.6392\n",
            "  Fold 1: aR²=0.6036\n",
            "Using top 3 folds: [1, 2, 4]\n",
            "\n",
            "Loading models...\n",
            "  Loaded: dinov3_best_fold1.pth\n",
            "  Loaded: dinov3_best_fold2.pth\n",
            "  Loaded: dinov3_best_fold4.pth\n",
            "\n",
            "Loading test data...\n",
            "Test samples: 357\n",
            "\n",
            "Running inference...\n",
            "TTA view 1/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TTA view 2/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TTA view 3/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions shape: (357, 5)\n",
            "\n",
            "Constraint check (G+D+C=T):\n",
            "  Max diff: 0.000015\n",
            "  Mean diff: 0.000002\n",
            "\n",
            "Prediction stats:\n",
            "  Green: mean=25.14, std=23.01, min=0.38, max=116.57\n",
            "  Dead: mean=12.48, std=7.48, min=0.00, max=34.58\n",
            "  Clover: mean=6.48, std=11.58, min=0.00, max=72.11\n",
            "  GDM: mean=31.62, std=23.21, min=0.46, max=117.48\n",
            "  Total: mean=44.10, std=24.85, min=0.46, max=130.69\n",
            "\n",
            "Creating submission...\n",
            "Saved: submission.csv\n",
            "                    sample_id      value\n",
            "0  ID1011485656__Dry_Clover_g   0.436817\n",
            "1    ID1011485656__Dry_Dead_g  25.848732\n",
            "2   ID1011485656__Dry_Green_g  19.265554\n",
            "3   ID1011485656__Dry_Total_g  45.551102\n",
            "4         ID1011485656__GDM_g  19.702368\n",
            "5  ID1012260530__Dry_Clover_g   0.114165\n",
            "6    ID1012260530__Dry_Dead_g   0.007654\n",
            "7   ID1012260530__Dry_Green_g   4.185601\n",
            "8   ID1012260530__Dry_Total_g   4.307419\n",
            "9         ID1012260530__GDM_g   4.299765\n",
            "                    sample_id      value\n",
            "0  ID1011485656__Dry_Clover_g   0.436817\n",
            "1    ID1011485656__Dry_Dead_g  25.848732\n",
            "2   ID1011485656__Dry_Green_g  19.265554\n",
            "3   ID1011485656__Dry_Total_g  45.551102\n",
            "4         ID1011485656__GDM_g  19.702368\n",
            "5  ID1012260530__Dry_Clover_g   0.114165\n",
            "6    ID1012260530__Dry_Dead_g   0.007654\n",
            "7   ID1012260530__Dry_Green_g   4.185601\n",
            "8   ID1012260530__Dry_Total_g   4.307419\n",
            "9         ID1012260530__GDM_g   4.299765\n",
            "\n",
            "============================================================\n",
            "Done!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    \"\"\"Configuration for DINOv3 Direct model inference.\"\"\"\n",
        "    \n",
        "    # ==================== LOCAL TESTING MODE ====================\n",
        "    LOCAL_TEST = True\n",
        "    \n",
        "    if LOCAL_TEST:\n",
        "        BASE_PATH = \"../data\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"train.csv\")  # Use train.csv for local OOF testing\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"train\")\n",
        "        # Trained DINOv3 model directory (from dinov3_train.py)\n",
        "        MODEL_DIR = \"/Users/kienvu/Desktop/kaggle/biomass/outputs/dinov3_20251218_174413\"  # Update to your model dir\n",
        "    else:\n",
        "        BASE_PATH = \"/kaggle/input/csiro-biomass\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"test\")\n",
        "        MODEL_DIR = \"/kaggle/input/your-dinov3-model\"  # Update for Kaggle\n",
        "    \n",
        "    # DINOv3 backbone (fixed)\n",
        "    BACKBONE = \"vit_base_patch16_dinov3\"\n",
        "    \n",
        "    # ==================== INFERENCE SETTINGS ====================\n",
        "    SUBMISSION_FILE = \"submission.csv\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Updated below\n",
        "    BATCH_SIZE = 1\n",
        "    NUM_WORKERS = 0\n",
        "    \n",
        "    # Model architecture params (auto-loaded from results.json)\n",
        "    DROPOUT = 0.3\n",
        "    HIDDEN_RATIO = 0.25\n",
        "    GRID = (2, 2)\n",
        "    USE_FILM = True\n",
        "    USE_ATTENTION_POOL = True\n",
        "    TRAIN_DEAD = False  # Whether model has head_dead\n",
        "    TRAIN_CLOVER = False  # Whether model has head_clover\n",
        "    USE_VEGETATION_INDICES = False  # Whether model uses VI features\n",
        "    USE_DISPARITY = False  # Whether model uses stereo disparity features\n",
        "    USE_LEARNABLE_AUG = False  # Whether model uses learnable augmentation\n",
        "    LEARNABLE_AUG_COLOR = True  # Learnable color augmentation\n",
        "    LEARNABLE_AUG_SPATIAL = False  # Learnable spatial augmentation\n",
        "    IMG_SIZE = 576  # DINOv3 default from dinov3_train.py\n",
        "    \n",
        "    # TTA settings\n",
        "    USE_TTA = True\n",
        "    \n",
        "    # Fold selection: None = use all folds, int = use top K folds by aR²\n",
        "    TOP_K_FOLDS: Optional[int] = 3  # e.g., 3 to use best 3 folds\n",
        "    \n",
        "    ALL_TARGET_COLS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
        "\n",
        "\n",
        "# DINOv3 uses 256 native resolution but accepts any size divisible by 16\n",
        "DINOV3_NATIVE_RES = 256\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Get best available device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Update device\n",
        "CFG.DEVICE = get_device()\n",
        "\n",
        "print(f\"Device: {CFG.DEVICE}\")\n",
        "\n",
        "\n",
        "def load_config_from_results(model_dir: str) -> Dict:\n",
        "    \"\"\"Load training config from results.json if available.\"\"\"\n",
        "    results_path = os.path.join(model_dir, \"results.json\")\n",
        "    if os.path.exists(results_path):\n",
        "        with open(results_path) as f:\n",
        "            results = json.load(f)\n",
        "        return results.get(\"config\", {})\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ==================== DATASET ====================\n",
        "\n",
        "class TestBiomassDataset(Dataset):\n",
        "    \"\"\"Dataset for test/inference.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, image_dir: str, transform: A.Compose):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:\n",
        "        row = self.df.iloc[idx]\n",
        "        sample_id = row[\"sample_id_prefix\"]\n",
        "        \n",
        "        img_path = os.path.join(self.image_dir, f\"{sample_id}.jpg\")\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        H, W = img.shape[:2]\n",
        "        mid = W // 2\n",
        "        img_left = img[:, :mid, :]\n",
        "        img_right = img[:, mid:, :]\n",
        "        \n",
        "        if self.transform:\n",
        "            aug_left = self.transform(image=img_left)\n",
        "            aug_right = self.transform(image=img_right)\n",
        "            img_left = aug_left[\"image\"]\n",
        "            img_right = aug_right[\"image\"]\n",
        "        \n",
        "        return img_left, img_right, sample_id\n",
        "\n",
        "\n",
        "def get_val_transform(img_size: Optional[int] = None) -> A.Compose:\n",
        "    \"\"\"Validation transform matching dinov3_train.py exactly.\"\"\"\n",
        "    if img_size is None:\n",
        "        img_size = CFG.IMG_SIZE\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_tta_transforms(img_size: Optional[int] = None) -> List[A.Compose]:\n",
        "    \"\"\"TTA transforms: original + hflip + brightness. Uses INTER_AREA to match training.\"\"\"\n",
        "    if img_size is None:\n",
        "        img_size = CFG.IMG_SIZE\n",
        "    base = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    hflip = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.HorizontalFlip(p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    bright = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    return [base, hflip, bright]\n",
        "\n",
        "\n",
        "# ==================== MODEL COMPONENTS ====================\n",
        "\n",
        "def build_dinov3_backbone(pretrained: bool = False) -> Tuple[nn.Module, int, int]:\n",
        "    \"\"\"Build DINOv3 backbone (vit_base_patch16_dinov3).\"\"\"\n",
        "    name = \"vit_base_patch16_dinov3\"\n",
        "    model = timm.create_model(name, pretrained=pretrained, num_classes=0)\n",
        "    feat_dim = model.num_features  # 768 for ViT-B\n",
        "    input_res = 256  # DINOv3 default\n",
        "    return model, feat_dim, input_res\n",
        "\n",
        "\n",
        "def _make_edges(length: int, n_parts: int) -> List[Tuple[int, int]]:\n",
        "    step = length // n_parts\n",
        "    edges = [(i * step, (i + 1) * step) for i in range(n_parts)]\n",
        "    edges[-1] = (edges[-1][0], length)\n",
        "    return edges\n",
        "\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, in_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        hidden = max(64, in_dim // 2)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, in_dim * 2),\n",
        "        )\n",
        "    \n",
        "    def forward(self, context: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        gb = self.mlp(context)\n",
        "        gamma, beta = torch.chunk(gb, 2, dim=1)\n",
        "        return gamma, beta\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.scale = dim ** -0.5\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        q = self.query(x.mean(dim=1, keepdim=True))\n",
        "        k = self.key(x)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        return (attn @ x).squeeze(1)\n",
        "\n",
        "\n",
        "# ==================== INNOVATIVE FEATURES ====================\n",
        "\n",
        "class VegetationIndices(nn.Module):\n",
        "    \"\"\"Compute vegetation indices (ExG, ExR, GRVI) from RGB image.\"\"\"\n",
        "    \n",
        "    def __init__(self, out_dim: int = 24) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(24, out_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1, 3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1, 3, 1, 1)\n",
        "        img_denorm = (img * std + mean).clamp(0, 1)\n",
        "        \n",
        "        r, g, b = img_denorm.unbind(dim=1)\n",
        "        exg = 2 * g - r - b\n",
        "        grvi = (g - r) / (g + r + 1e-6)\n",
        "        vari = (g - r) / (g + r - b + 1e-6)\n",
        "        exr = 1.4 * r - g\n",
        "        exgr = exg - exr\n",
        "        norm_g = g / (r + g + b + 1e-6)\n",
        "        \n",
        "        indices = torch.stack([exg, exr, exgr, grvi, norm_g, vari], dim=1)\n",
        "        feats = []\n",
        "        for i in range(indices.size(1)):\n",
        "            idx = indices[:, i]\n",
        "            feats.extend([\n",
        "                idx.mean(dim=(-2, -1)),\n",
        "                idx.std(dim=(-2, -1)),\n",
        "                idx.flatten(1).quantile(0.1, dim=1),\n",
        "                idx.flatten(1).quantile(0.9, dim=1),\n",
        "            ])\n",
        "        stats = torch.stack(feats, dim=1)\n",
        "        return self.proj(stats)\n",
        "\n",
        "\n",
        "class DisparityFeatures(nn.Module):\n",
        "    \"\"\"Extract stereo disparity features from tile features.\"\"\"\n",
        "    \n",
        "    def __init__(self, feat_dim: int, max_disparity: int = 8, out_dim: int = None) -> None:\n",
        "        super().__init__()\n",
        "        self.max_disparity = max_disparity\n",
        "        if out_dim is None:\n",
        "            out_dim = feat_dim // 4\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(max_disparity, out_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.diff_proj = nn.Sequential(\n",
        "            nn.Linear(6, out_dim // 2),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.out_dim = out_dim + out_dim // 2\n",
        "    \n",
        "    def forward(self, feat_left: torch.Tensor, feat_right: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, D = feat_left.shape\n",
        "        feat_l = F.normalize(feat_left, dim=-1)\n",
        "        feat_r = F.normalize(feat_right, dim=-1)\n",
        "        \n",
        "        correlations = []\n",
        "        for d in range(self.max_disparity):\n",
        "            shifted_r = torch.roll(feat_r, shifts=d, dims=1)\n",
        "            corr = (feat_l * shifted_r).sum(dim=-1).mean(dim=1)\n",
        "            correlations.append(corr)\n",
        "        corr_volume = torch.stack(correlations, dim=-1)\n",
        "        corr_feat = self.proj(corr_volume)\n",
        "        \n",
        "        fl_pooled = feat_left.mean(dim=1)\n",
        "        fr_pooled = feat_right.mean(dim=1)\n",
        "        fl_norm = F.normalize(fl_pooled, dim=-1)\n",
        "        fr_norm = F.normalize(fr_pooled, dim=-1)\n",
        "        \n",
        "        correlation = (fl_norm * fr_norm).sum(dim=-1, keepdim=True)\n",
        "        diff = fl_pooled - fr_pooled\n",
        "        diff_norm = diff.norm(dim=-1, keepdim=True)\n",
        "        diff_mean = diff.mean(dim=-1, keepdim=True)\n",
        "        diff_std = diff.std(dim=-1, keepdim=True)\n",
        "        ratio = fl_pooled / (fr_pooled + 1e-6)\n",
        "        ratio_mean = ratio.mean(dim=-1, keepdim=True)\n",
        "        ratio_std = ratio.std(dim=-1, keepdim=True)\n",
        "        \n",
        "        stats = torch.cat([correlation, diff_norm, diff_mean, diff_std, ratio_mean, ratio_std], dim=-1)\n",
        "        diff_feat = self.diff_proj(stats)\n",
        "        return torch.cat([corr_feat, diff_feat], dim=-1)\n",
        "\n",
        "\n",
        "class LearnableAugmentation(nn.Module):\n",
        "    \"\"\"Learnable augmentation module (identity at inference time).\n",
        "    \n",
        "    Includes all strong augmentations: color, spatial, blur, CLAHE.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        enable_color: bool = True,\n",
        "        enable_spatial: bool = False,\n",
        "        enable_blur: bool = True,\n",
        "        enable_local_contrast: bool = True,\n",
        "        color_strength: float = 0.25,\n",
        "        spatial_strength: float = 0.15,\n",
        "        noise_std: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.enable_color = enable_color\n",
        "        self.enable_spatial = enable_spatial\n",
        "        self.enable_blur = enable_blur\n",
        "        self.enable_local_contrast = enable_local_contrast\n",
        "        \n",
        "        if enable_color:\n",
        "            self.color_params = nn.Parameter(torch.zeros(6))\n",
        "            self.color_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 32),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(32, 6),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "        \n",
        "        if enable_spatial:\n",
        "            self.spatial_params = nn.Parameter(torch.zeros(5))\n",
        "            self.spatial_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 32),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(32, 5),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "        \n",
        "        if enable_blur:\n",
        "            self.blur_params = nn.Parameter(torch.zeros(2))\n",
        "            self.blur_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 16),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(16, 2),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "            self.blur_kernel = nn.Parameter(torch.tensor([\n",
        "                [1., 2., 1.],\n",
        "                [2., 4., 2.],\n",
        "                [1., 2., 1.],\n",
        "            ]) / 16.0)\n",
        "        \n",
        "        if enable_local_contrast:\n",
        "            self.contrast_params = nn.Parameter(torch.zeros(2))\n",
        "            self.contrast_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 16),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(16, 2),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "    \n",
        "    def forward(self, img: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Identity during inference (eval mode)\n",
        "        return img, torch.tensor(0.0, device=img.device)\n",
        "\n",
        "\n",
        "# ==================== DINOv3 DIRECT MODEL ====================\n",
        "\n",
        "class DINOv3Direct(nn.Module):\n",
        "    \"\"\"\n",
        "    DINOv3 Direct Model for Biomass Prediction.\n",
        "    Predicts Total, Green, GDM directly; optionally predicts Dead, Clover.\n",
        "    Components always sum to Total (mathematically guaranteed).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        grid: Tuple[int, int] = (2, 2),\n",
        "        pretrained: bool = False,\n",
        "        dropout: float = 0.3,\n",
        "        hidden_ratio: float = 0.25,\n",
        "        use_film: bool = True,\n",
        "        use_attention_pool: bool = True,\n",
        "        train_dead: bool = False,\n",
        "        train_clover: bool = False,\n",
        "        use_vegetation_indices: bool = False,\n",
        "        use_disparity: bool = False,\n",
        "        use_learnable_aug: bool = False,\n",
        "        learnable_aug_color: bool = True,\n",
        "        learnable_aug_spatial: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        # Build DINOv3 backbone\n",
        "        self.backbone, self.feat_dim, self.input_res = build_dinov3_backbone(pretrained)\n",
        "        self.grid = tuple(grid)\n",
        "        self.use_film = use_film\n",
        "        self.use_attention_pool = use_attention_pool\n",
        "        self.train_dead = train_dead\n",
        "        self.train_clover = train_clover\n",
        "        self.use_vegetation_indices = use_vegetation_indices\n",
        "        self.use_disparity = use_disparity\n",
        "        self.use_learnable_aug = use_learnable_aug\n",
        "        \n",
        "        # Learnable augmentation (identity at inference)\n",
        "        # Includes all strong augmentations: color, spatial, blur, CLAHE\n",
        "        if use_learnable_aug:\n",
        "            self.learnable_aug_left = LearnableAugmentation(\n",
        "                enable_color=learnable_aug_color,\n",
        "                enable_spatial=learnable_aug_spatial,\n",
        "                enable_blur=learnable_aug_color,\n",
        "                enable_local_contrast=learnable_aug_color,\n",
        "            )\n",
        "            self.learnable_aug_right = LearnableAugmentation(\n",
        "                enable_color=learnable_aug_color,\n",
        "                enable_spatial=learnable_aug_spatial,\n",
        "                enable_blur=learnable_aug_color,\n",
        "                enable_local_contrast=learnable_aug_color,\n",
        "            )\n",
        "        \n",
        "        # FiLM for cross-view conditioning\n",
        "        if use_film:\n",
        "            self.film_left = FiLM(self.feat_dim)\n",
        "            self.film_right = FiLM(self.feat_dim)\n",
        "        \n",
        "        # Attention pooling for tiles\n",
        "        if use_attention_pool:\n",
        "            self.attn_pool_left = AttentionPooling(self.feat_dim)\n",
        "            self.attn_pool_right = AttentionPooling(self.feat_dim)\n",
        "        \n",
        "        # === Optional feature modules ===\n",
        "        extra_dim = 0\n",
        "        if use_vegetation_indices:\n",
        "            vi_out = 24\n",
        "            self.vi_left = VegetationIndices(out_dim=vi_out)\n",
        "            self.vi_right = VegetationIndices(out_dim=vi_out)\n",
        "            extra_dim += vi_out * 2\n",
        "        \n",
        "        if use_disparity:\n",
        "            self.disparity_module = DisparityFeatures(self.feat_dim, max_disparity=8, out_dim=self.feat_dim // 4)\n",
        "            extra_dim += self.disparity_module.out_dim\n",
        "        \n",
        "        # Head dimensions\n",
        "        combined_dim = self.feat_dim * 2 + extra_dim\n",
        "        hidden_dim = max(64, int((self.feat_dim * 2) * hidden_ratio))\n",
        "        \n",
        "        # Shared projection\n",
        "        self.shared_proj = nn.Sequential(\n",
        "            nn.LayerNorm(combined_dim),\n",
        "            nn.Linear(combined_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "        # Prediction heads\n",
        "        def _make_head() -> nn.Sequential:\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout * 0.5),\n",
        "                nn.Linear(hidden_dim, 1),\n",
        "            )\n",
        "        \n",
        "        self.head_total = _make_head()\n",
        "        self.head_green = _make_head()\n",
        "        self.head_gdm = _make_head()\n",
        "        \n",
        "        # Optional heads for Dead and Clover\n",
        "        self.head_dead = _make_head() if train_dead else None\n",
        "        self.head_clover = _make_head() if train_clover else None\n",
        "        \n",
        "        self.softplus = nn.Softplus(beta=1.0)\n",
        "    \n",
        "    def _collect_tiles(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        \"\"\"Split image into grid of tiles.\"\"\"\n",
        "        _, _, H, W = x.shape\n",
        "        r, c = self.grid\n",
        "        rows = _make_edges(H, r)\n",
        "        cols = _make_edges(W, c)\n",
        "        \n",
        "        tiles = []\n",
        "        for rs, re in rows:\n",
        "            for cs, ce in cols:\n",
        "                tile = x[:, :, rs:re, cs:ce]\n",
        "                if tile.shape[-2:] != (self.input_res, self.input_res):\n",
        "                    tile = F.interpolate(\n",
        "                        tile,\n",
        "                        size=(self.input_res, self.input_res),\n",
        "                        mode=\"bilinear\",\n",
        "                        align_corners=False,\n",
        "                    )\n",
        "                tiles.append(tile)\n",
        "        return tiles\n",
        "    \n",
        "    def _extract_features(\n",
        "        self, x_left: torch.Tensor, x_right: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract tile features from both views in one backbone call.\"\"\"\n",
        "        B = x_left.size(0)\n",
        "        \n",
        "        tiles_left = self._collect_tiles(x_left)\n",
        "        tiles_right = self._collect_tiles(x_right)\n",
        "        num_tiles = len(tiles_left)\n",
        "        \n",
        "        # Process all tiles in one forward pass\n",
        "        all_tiles = torch.cat(tiles_left + tiles_right, dim=0)\n",
        "        all_feats = self.backbone(all_tiles)\n",
        "        \n",
        "        # Reshape\n",
        "        all_feats = all_feats.view(2 * num_tiles, B, -1).permute(1, 0, 2)\n",
        "        feats_left = all_feats[:, :num_tiles, :]\n",
        "        feats_right = all_feats[:, num_tiles:, :]\n",
        "        \n",
        "        return feats_left, feats_right\n",
        "    \n",
        "    def forward(\n",
        "        self, x_left: torch.Tensor, x_right: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        # Extract tile features\n",
        "        tiles_left, tiles_right = self._extract_features(x_left, x_right)\n",
        "        \n",
        "        # Stereo Disparity Features (before FiLM)\n",
        "        disp_feat = None\n",
        "        if self.use_disparity:\n",
        "            disp_feat = self.disparity_module(tiles_left, tiles_right)\n",
        "        \n",
        "        # Context for FiLM\n",
        "        ctx_left = tiles_left.mean(dim=1)\n",
        "        ctx_right = tiles_right.mean(dim=1)\n",
        "        \n",
        "        # Apply FiLM cross-conditioning\n",
        "        if self.use_film:\n",
        "            gamma_l, beta_l = self.film_left(ctx_right)\n",
        "            gamma_r, beta_r = self.film_right(ctx_left)\n",
        "            tiles_left = tiles_left * (1 + gamma_l.unsqueeze(1)) + beta_l.unsqueeze(1)\n",
        "            tiles_right = tiles_right * (1 + gamma_r.unsqueeze(1)) + beta_r.unsqueeze(1)\n",
        "        \n",
        "        # Pool tiles\n",
        "        if self.use_attention_pool:\n",
        "            f_left = self.attn_pool_left(tiles_left)\n",
        "            f_right = self.attn_pool_right(tiles_right)\n",
        "        else:\n",
        "            f_left = tiles_left.mean(dim=1)\n",
        "            f_right = tiles_right.mean(dim=1)\n",
        "        \n",
        "        # Combine features\n",
        "        features_list = [f_left, f_right]\n",
        "        \n",
        "        if self.use_vegetation_indices:\n",
        "            vi_left = self.vi_left(x_left)\n",
        "            vi_right = self.vi_right(x_right)\n",
        "            features_list.extend([vi_left, vi_right])\n",
        "        \n",
        "        if disp_feat is not None:\n",
        "            features_list.append(disp_feat)\n",
        "        \n",
        "        f = torch.cat(features_list, dim=1)\n",
        "        f = self.shared_proj(f)\n",
        "        \n",
        "        # Core predictions\n",
        "        total_raw = self.softplus(self.head_total(f))\n",
        "        green_raw = self.softplus(self.head_green(f))\n",
        "        gdm_raw = self.softplus(self.head_gdm(f))\n",
        "        \n",
        "        # Enforce constraints: Total >= GDM >= Green\n",
        "        total = total_raw\n",
        "        gdm = torch.minimum(gdm_raw, total)\n",
        "        green = torch.minimum(green_raw, gdm)\n",
        "        \n",
        "        # Dead: predicted or derived\n",
        "        if self.head_dead is not None:\n",
        "            dead_raw = self.softplus(self.head_dead(f))\n",
        "            dead = torch.minimum(dead_raw, total - gdm + 1e-6)\n",
        "            dead = F.relu(dead)\n",
        "        else:\n",
        "            dead = F.relu(total - gdm)\n",
        "        \n",
        "        # Clover: predicted or derived\n",
        "        if self.head_clover is not None:\n",
        "            clover_raw = self.softplus(self.head_clover(f))\n",
        "            clover = torch.minimum(clover_raw, gdm - green + 1e-6)\n",
        "            clover = F.relu(clover)\n",
        "        else:\n",
        "            clover = F.relu(gdm - green)\n",
        "        \n",
        "        return green, dead, clover, gdm, total\n",
        "\n",
        "\n",
        "# ==================== MODEL LOADING ====================\n",
        "\n",
        "def _strip_module_prefix(sd: dict) -> dict:\n",
        "    \"\"\"Remove 'module.' prefix from state dict keys (for DDP-trained models).\"\"\"\n",
        "    if not sd:\n",
        "        return sd\n",
        "    keys = list(sd.keys())\n",
        "    if all(k.startswith(\"module.\") for k in keys):\n",
        "        return {k[len(\"module.\"):]: v for k, v in sd.items()}\n",
        "    return sd\n",
        "\n",
        "\n",
        "def _detect_model_config(sd_keys: set) -> dict:\n",
        "    \"\"\"Auto-detect model config from checkpoint keys.\"\"\"\n",
        "    # Detect learnable aug color/spatial from keys\n",
        "    has_learnable_aug = any(k.startswith(\"learnable_aug_left.\") for k in sd_keys)\n",
        "    learnable_aug_color = any(\"color_params\" in k for k in sd_keys if k.startswith(\"learnable_aug_left.\"))\n",
        "    learnable_aug_spatial = any(\"spatial_params\" in k for k in sd_keys if k.startswith(\"learnable_aug_left.\"))\n",
        "    \n",
        "    return {\n",
        "        \"use_film\": any(k.startswith(\"film_left.\") for k in sd_keys),\n",
        "        \"use_attention_pool\": any(k.startswith(\"attn_pool_left.\") for k in sd_keys),\n",
        "        \"train_dead\": any(k.startswith(\"head_dead.\") for k in sd_keys),\n",
        "        \"train_clover\": any(k.startswith(\"head_clover.\") for k in sd_keys),\n",
        "        \"use_vegetation_indices\": any(k.startswith(\"vi_left.\") for k in sd_keys),\n",
        "        \"use_disparity\": any(k.startswith(\"disparity_module.\") for k in sd_keys),\n",
        "        \"use_learnable_aug\": has_learnable_aug,\n",
        "        \"learnable_aug_color\": learnable_aug_color,\n",
        "        \"learnable_aug_spatial\": learnable_aug_spatial,\n",
        "    }\n",
        "\n",
        "\n",
        "def load_fold_model(\n",
        "    path: str,\n",
        "    grid: Tuple[int, int] = (2, 2),\n",
        "    dropout: float = 0.3,\n",
        "    hidden_ratio: float = 0.25,\n",
        "    use_film: bool = True,\n",
        "    use_attention_pool: bool = True,\n",
        "    train_dead: bool = False,\n",
        "    train_clover: bool = False,\n",
        "    use_vegetation_indices: bool = False,\n",
        "    use_disparity: bool = False,\n",
        "    use_learnable_aug: bool = False,\n",
        "    learnable_aug_color: bool = True,\n",
        "    learnable_aug_spatial: bool = False,\n",
        ") -> nn.Module:\n",
        "    \"\"\"Load a DINOv3Direct model checkpoint.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
        "    \n",
        "    try:\n",
        "        raw_sd = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
        "    except TypeError:\n",
        "        raw_sd = torch.load(path, map_location=\"cpu\")\n",
        "    \n",
        "    sd = _strip_module_prefix(raw_sd)\n",
        "    sd_keys = set(sd.keys())\n",
        "    \n",
        "    # Auto-detect config from checkpoint\n",
        "    detected_config = _detect_model_config(sd_keys)\n",
        "    use_film = detected_config.get(\"use_film\", use_film)\n",
        "    use_attention_pool = detected_config.get(\"use_attention_pool\", use_attention_pool)\n",
        "    train_dead = detected_config.get(\"train_dead\", train_dead)\n",
        "    train_clover = detected_config.get(\"train_clover\", train_clover)\n",
        "    use_vegetation_indices = detected_config.get(\"use_vegetation_indices\", use_vegetation_indices)\n",
        "    use_disparity = detected_config.get(\"use_disparity\", use_disparity)\n",
        "    use_learnable_aug = detected_config.get(\"use_learnable_aug\", use_learnable_aug)\n",
        "    learnable_aug_color = detected_config.get(\"learnable_aug_color\", learnable_aug_color)\n",
        "    learnable_aug_spatial = detected_config.get(\"learnable_aug_spatial\", learnable_aug_spatial)\n",
        "    \n",
        "    # DINOv3Direct model (from dinov3_train.py)\n",
        "    model = DINOv3Direct(\n",
        "        grid=grid,\n",
        "        pretrained=False,\n",
        "        dropout=dropout,\n",
        "        hidden_ratio=hidden_ratio,\n",
        "        use_film=use_film,\n",
        "        use_attention_pool=use_attention_pool,\n",
        "        train_dead=train_dead,\n",
        "        train_clover=train_clover,\n",
        "        use_vegetation_indices=use_vegetation_indices,\n",
        "        use_disparity=use_disparity,\n",
        "        use_learnable_aug=use_learnable_aug,\n",
        "        learnable_aug_color=learnable_aug_color,\n",
        "        learnable_aug_spatial=learnable_aug_spatial,\n",
        "    )\n",
        "    \n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    model = model.to(CFG.DEVICE)\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def find_model_checkpoints(model_dir: str, top_k: Optional[int] = None) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Find DINOv3 model checkpoints (dinov3_best_fold*.pth).\n",
        "    \n",
        "    Args:\n",
        "        model_dir: Directory containing model checkpoints\n",
        "        top_k: If set, return only top K folds by aR² (from results.json)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (checkpoint_paths, fold_indices)\n",
        "    \"\"\"\n",
        "    # Find all checkpoints\n",
        "    all_checkpoints = []\n",
        "    for f in os.listdir(model_dir):\n",
        "        if f.startswith(\"dinov3_best_fold\") and f.endswith(\".pth\"):\n",
        "            # Extract fold number from filename\n",
        "            fold_num = int(f.replace(\"dinov3_best_fold\", \"\").replace(\".pth\", \"\"))\n",
        "            all_checkpoints.append((fold_num, os.path.join(model_dir, f)))\n",
        "    \n",
        "    all_checkpoints.sort(key=lambda x: x[0])  # Sort by fold number\n",
        "    \n",
        "    if top_k is None or top_k >= len(all_checkpoints):\n",
        "        # Return all checkpoints\n",
        "        return [c[1] for c in all_checkpoints], [c[0] for c in all_checkpoints]\n",
        "    \n",
        "    # Load results.json to get fold metrics\n",
        "    results_path = os.path.join(model_dir, \"results.json\")\n",
        "    if not os.path.exists(results_path):\n",
        "        print(f\"Warning: results.json not found, using all {len(all_checkpoints)} folds\")\n",
        "        return [c[1] for c in all_checkpoints], [c[0] for c in all_checkpoints]\n",
        "    \n",
        "    with open(results_path) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    fold_results = results.get(\"folds\", [])\n",
        "    if not fold_results:\n",
        "        print(f\"Warning: No fold results in results.json, using all {len(all_checkpoints)} folds\")\n",
        "        return [c[1] for c in all_checkpoints], [c[0] for c in all_checkpoints]\n",
        "    \n",
        "    # Get fold metrics (aR² = best_r2 after our change, or avg_r2 from metrics)\n",
        "    fold_metrics = []\n",
        "    for fr in fold_results:\n",
        "        fold_num = fr[\"fold\"]\n",
        "        # best_r2 is now aR² after our dinov3_train.py change\n",
        "        # For older models, try metrics.avg_r2, then fall back to best_r2\n",
        "        metrics = fr.get(\"metrics\", {})\n",
        "        ar2 = metrics.get(\"avg_r2\", fr.get(\"best_r2\", 0))\n",
        "        fold_metrics.append((fold_num, ar2))\n",
        "    \n",
        "    # Sort by metric (descending) and take top K\n",
        "    fold_metrics.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_folds = set(fm[0] for fm in fold_metrics[:top_k])\n",
        "    \n",
        "    print(f\"Selecting top {top_k} folds by aR²:\")\n",
        "    for fold_num, ar2 in fold_metrics[:top_k]:\n",
        "        print(f\"  Fold {fold_num}: aR²={ar2:.4f}\")\n",
        "    \n",
        "    # Filter checkpoints to top folds\n",
        "    selected = [(fold_num, path) for fold_num, path in all_checkpoints if fold_num in top_folds]\n",
        "    selected.sort(key=lambda x: x[0])\n",
        "    \n",
        "    return [c[1] for c in selected], [c[0] for c in selected]\n",
        "\n",
        "\n",
        "# ==================== INFERENCE ====================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one_view(models: List[nn.Module], loader: DataLoader) -> np.ndarray:\n",
        "    \"\"\"Run inference with fold ensemble.\"\"\"\n",
        "    out_list = []\n",
        "    \n",
        "    for batch in tqdm(loader, desc=\"Predicting\", leave=False):\n",
        "        x_left, x_right, _ = batch\n",
        "        x_left = x_left.to(CFG.DEVICE)\n",
        "        x_right = x_right.to(CFG.DEVICE)\n",
        "        \n",
        "        fold_preds = []\n",
        "        for model in models:\n",
        "            outputs = model(x_left, x_right)\n",
        "            # Handle both old (5 outputs) and new (6 outputs with aux_loss) models\n",
        "            if len(outputs) == 6:\n",
        "                green, dead, clover, gdm, total, _ = outputs\n",
        "            else:\n",
        "                green, dead, clover, gdm, total = outputs\n",
        "            pred = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
        "            fold_preds.append(pred.float().cpu().numpy())\n",
        "        \n",
        "        avg_pred = np.mean(fold_preds, axis=0)\n",
        "        out_list.append(avg_pred)\n",
        "    \n",
        "    return np.concatenate(out_list, axis=0)\n",
        "\n",
        "\n",
        "def run_inference_tta(models: List[nn.Module], df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Run inference with TTA.\"\"\"\n",
        "    transforms = get_tta_transforms() if CFG.USE_TTA else [get_val_transform()]\n",
        "    \n",
        "    all_preds = []\n",
        "    for i, transform in enumerate(transforms):\n",
        "        print(f\"TTA view {i+1}/{len(transforms)}...\")\n",
        "        ds = TestBiomassDataset(df, CFG.TEST_IMAGE_DIR, transform)\n",
        "        loader = DataLoader(\n",
        "            ds, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS\n",
        "        )\n",
        "        preds = predict_one_view(models, loader)\n",
        "        all_preds.append(preds)\n",
        "    \n",
        "    return np.mean(all_preds, axis=0)\n",
        "\n",
        "\n",
        "def create_submission(final_5: np.ndarray, test_long: pd.DataFrame, test_unique: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Create submission DataFrame with order matching test_long.\"\"\"\n",
        "    green = final_5[:, 0]\n",
        "    dead = final_5[:, 1]\n",
        "    clover = final_5[:, 2]\n",
        "    gdm = final_5[:, 3]\n",
        "    total = final_5[:, 4]\n",
        "\n",
        "    def nnz(x: np.ndarray) -> np.ndarray:\n",
        "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    green, dead, clover, gdm, total = map(nnz, [green, dead, clover, gdm, total])\n",
        "\n",
        "    wide = pd.DataFrame(\n",
        "        {\n",
        "            \"image_path\": test_unique[\"image_path\"],\n",
        "            \"Dry_Green_g\": green,\n",
        "            \"Dry_Dead_g\": dead,\n",
        "            \"Dry_Clover_g\": clover,\n",
        "            \"GDM_g\": gdm,\n",
        "            \"Dry_Total_g\": total,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    long_preds = wide.melt(\n",
        "        id_vars=[\"image_path\"],\n",
        "        value_vars=CFG.ALL_TARGET_COLS,\n",
        "        var_name=\"target_name\",\n",
        "        value_name=\"target\",\n",
        "    )\n",
        "\n",
        "    sub = pd.merge(\n",
        "        test_long[[\"sample_id\", \"image_path\", \"target_name\"]],\n",
        "        long_preds,\n",
        "        on=[\"image_path\", \"target_name\"],\n",
        "        how=\"left\",\n",
        "    )[[\"sample_id\", \"target\"]]\n",
        "\n",
        "    sub[\"target\"] = np.nan_to_num(sub[\"target\"], nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sub.columns = [\"sample_id\", \"value\"]  # Rename for submission format\n",
        "    sub.to_csv(CFG.SUBMISSION_FILE, index=False)\n",
        "    print(f\"Saved: {CFG.SUBMISSION_FILE}\")\n",
        "    print(sub.head(10))\n",
        "    return sub, wide\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run DINOv3 Direct model inference.\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"DINOv3 Direct Model Inference\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Device: {CFG.DEVICE}\")\n",
        "    print(f\"Model dir: {CFG.MODEL_DIR}\")\n",
        "    print(f\"TTA: {CFG.USE_TTA}\")\n",
        "    \n",
        "    # Load config from results.json (dinov3_train.py output)\n",
        "    config = load_config_from_results(CFG.MODEL_DIR)\n",
        "    if config:\n",
        "        # dinov3_train.py saves grid as single int\n",
        "        grid_val = config.get(\"grid\", 2)\n",
        "        CFG.GRID = (grid_val, grid_val) if isinstance(grid_val, int) else tuple(grid_val)\n",
        "        CFG.DROPOUT = config.get(\"dropout\", CFG.DROPOUT)\n",
        "        CFG.HIDDEN_RATIO = config.get(\"hidden_ratio\", CFG.HIDDEN_RATIO)\n",
        "        CFG.USE_FILM = config.get(\"use_film\", CFG.USE_FILM)\n",
        "        CFG.USE_ATTENTION_POOL = config.get(\"use_attention_pool\", CFG.USE_ATTENTION_POOL)\n",
        "        CFG.TRAIN_DEAD = config.get(\"train_dead\", CFG.TRAIN_DEAD)\n",
        "        CFG.TRAIN_CLOVER = config.get(\"train_clover\", CFG.TRAIN_CLOVER)\n",
        "        CFG.USE_VEGETATION_INDICES = config.get(\"use_vegetation_indices\", CFG.USE_VEGETATION_INDICES)\n",
        "        CFG.USE_DISPARITY = config.get(\"use_disparity\", CFG.USE_DISPARITY)\n",
        "        CFG.USE_LEARNABLE_AUG = config.get(\"use_learnable_aug\", CFG.USE_LEARNABLE_AUG)\n",
        "        CFG.LEARNABLE_AUG_COLOR = config.get(\"learnable_aug_color\", CFG.LEARNABLE_AUG_COLOR)\n",
        "        CFG.LEARNABLE_AUG_SPATIAL = config.get(\"learnable_aug_spatial\", CFG.LEARNABLE_AUG_SPATIAL)\n",
        "        print(f\"Loaded config from results.json\")\n",
        "    \n",
        "    print(f\"Model: DINOv3Direct\")\n",
        "    print(f\"Backbone: vit_base_patch16_dinov3\")\n",
        "    print(f\"Grid: {CFG.GRID}\")\n",
        "    print(f\"Image size: {CFG.IMG_SIZE}\")\n",
        "    if CFG.TRAIN_DEAD or CFG.TRAIN_CLOVER:\n",
        "        print(f\"Optional heads: train_dead={CFG.TRAIN_DEAD}, train_clover={CFG.TRAIN_CLOVER}\")\n",
        "    if CFG.USE_VEGETATION_INDICES or CFG.USE_DISPARITY or CFG.USE_LEARNABLE_AUG:\n",
        "        extras = []\n",
        "        if CFG.USE_VEGETATION_INDICES:\n",
        "            extras.append(\"Vegetation Indices\")\n",
        "        if CFG.USE_DISPARITY:\n",
        "            extras.append(\"Stereo Disparity\")\n",
        "        if CFG.USE_LEARNABLE_AUG:\n",
        "            aug_types = []\n",
        "            if CFG.LEARNABLE_AUG_COLOR:\n",
        "                aug_types.append(\"color\")\n",
        "            if CFG.LEARNABLE_AUG_SPATIAL:\n",
        "                aug_types.append(\"spatial\")\n",
        "            extras.append(f\"Learnable Aug ({'+'.join(aug_types)})\")\n",
        "        print(f\"Innovative features: {', '.join(extras)}\")\n",
        "    \n",
        "    # Find checkpoints (optionally select top K folds)\n",
        "    checkpoints, fold_indices = find_model_checkpoints(CFG.MODEL_DIR, top_k=CFG.TOP_K_FOLDS)\n",
        "    if not checkpoints:\n",
        "        raise ValueError(f\"No checkpoints found in {CFG.MODEL_DIR}\")\n",
        "    if CFG.TOP_K_FOLDS:\n",
        "        print(f\"Using top {len(checkpoints)} folds: {fold_indices}\")\n",
        "    else:\n",
        "        print(f\"Found {len(checkpoints)} fold checkpoints\")\n",
        "    \n",
        "    # Load models\n",
        "    print(\"\\nLoading models...\")\n",
        "    models = []\n",
        "    for ckpt in checkpoints:\n",
        "        model = load_fold_model(\n",
        "            ckpt,\n",
        "            grid=CFG.GRID,\n",
        "            dropout=CFG.DROPOUT,\n",
        "            hidden_ratio=CFG.HIDDEN_RATIO,\n",
        "            use_film=CFG.USE_FILM,\n",
        "            use_attention_pool=CFG.USE_ATTENTION_POOL,\n",
        "            train_dead=CFG.TRAIN_DEAD,\n",
        "            train_clover=CFG.TRAIN_CLOVER,\n",
        "            use_vegetation_indices=CFG.USE_VEGETATION_INDICES,\n",
        "            use_disparity=CFG.USE_DISPARITY,\n",
        "            use_learnable_aug=CFG.USE_LEARNABLE_AUG,\n",
        "            learnable_aug_color=CFG.LEARNABLE_AUG_COLOR,\n",
        "            learnable_aug_spatial=CFG.LEARNABLE_AUG_SPATIAL,\n",
        "        )\n",
        "        models.append(model)\n",
        "        print(f\"  Loaded: {os.path.basename(ckpt)}\")\n",
        "    \n",
        "    # Load test data\n",
        "    print(\"\\nLoading test data...\")\n",
        "    test_long = pd.read_csv(CFG.TEST_CSV)\n",
        "    \n",
        "    # Extract sample_id_prefix for dataset compatibility\n",
        "    if \"sample_id_prefix\" not in test_long.columns:\n",
        "        if \"image_path\" in test_long.columns:\n",
        "            test_long[\"sample_id_prefix\"] = test_long[\"image_path\"].str.extract(r'([A-Z]+\\d+)')[0]\n",
        "        elif \"sample_id\" in test_long.columns:\n",
        "            test_long[\"sample_id_prefix\"] = test_long[\"sample_id\"].str.split(\"__\").str[0]\n",
        "        else:\n",
        "            raise ValueError(\"Cannot find sample_id or image_path column\")\n",
        "    \n",
        "    # Use image_path for dedup to match v1 submission ordering\n",
        "    test_unique = test_long.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
        "    print(f\"Test samples: {len(test_unique)}\")\n",
        "    \n",
        "    # Run inference\n",
        "    print(\"\\nRunning inference...\")\n",
        "    preds = run_inference_tta(models, test_unique)\n",
        "    print(f\"Predictions shape: {preds.shape}\")\n",
        "    \n",
        "    # Constraint check\n",
        "    component_sum = preds[:, 0] + preds[:, 1] + preds[:, 2]\n",
        "    total_pred = preds[:, 4]\n",
        "    diff = np.abs(component_sum - total_pred)\n",
        "    print(f\"\\nConstraint check (G+D+C=T):\")\n",
        "    print(f\"  Max diff: {diff.max():.6f}\")\n",
        "    print(f\"  Mean diff: {diff.mean():.6f}\")\n",
        "    \n",
        "    # Statistics\n",
        "    print(f\"\\nPrediction stats:\")\n",
        "    for i, name in enumerate([\"Green\", \"Dead\", \"Clover\", \"GDM\", \"Total\"]):\n",
        "        print(f\"  {name}: mean={preds[:, i].mean():.2f}, std={preds[:, i].std():.2f}, \"\n",
        "              f\"min={preds[:, i].min():.2f}, max={preds[:, i].max():.2f}\")\n",
        "    \n",
        "    # Create submission\n",
        "    print(\"\\nCreating submission...\")\n",
        "    sub_df, wide_df = create_submission(preds, test_long, test_unique)\n",
        "    print(sub_df.head(10))\n",
        "    \n",
        "    # Cleanup\n",
        "    del models\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Done!\")\n",
        "    print(\"=\"*60)\n",
        "    return sub_df\n",
        "\n",
        "\n",
        "# Run\n",
        "submission = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
