{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3612a7eb",
      "metadata": {
        "papermill": {
          "duration": 0.002351,
          "end_time": "2025-11-26T12:39:56.782286",
          "exception": false,
          "start_time": "2025-11-26T12:39:56.779935",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## CSIRO Image2Biomass â€“ Dual Ensemble Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee52cf8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-26T12:39:56.787569Z",
          "iopub.status.busy": "2025-11-26T12:39:56.787301Z",
          "iopub.status.idle": "2025-11-26T12:41:55.526213Z",
          "shell.execute_reply": "2025-11-26T12:41:55.525318Z"
        },
        "papermill": {
          "duration": 118.743613,
          "end_time": "2025-11-26T12:41:55.527607",
          "exception": false,
          "start_time": "2025-11-26T12:39:56.783994",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Backbone A: vit_base_patch14_reg4_dinov2.lvd142m (weight=0.8)\n",
            "Backbone B: vit_small_patch14_dinov2.lvd142m (weight=0.2)\n",
            "\n",
            "================= Loading test data =================\n",
            "Found 357 unique test images.\n",
            "\n",
            "================= Backbone A inference =================\n",
            "\n",
            "================= Loading vit_base_patch14_reg4_dinov2.lvd142m (5 folds) =================\n",
            "Model dir: /root/workspace/outputs/stage1_base\n",
            "Found 5 checkpoints\n",
            "fold0 => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2.lvd142m, input_res=518\n",
            "tiled_film_best_model_fold1.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2.lvd142m, input_res=518\n",
            "tiled_film_best_model_fold2.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2.lvd142m, input_res=518\n",
            "tiled_film_best_model_fold3.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2.lvd142m, input_res=518\n",
            "tiled_film_best_model_fold4.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2.lvd142m, input_res=518\n",
            "\n",
            "--- TTA view 1/3 (resize=518) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- TTA view 2/3 (resize=518) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- TTA view 3/3 (resize=518) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================= Backbone B inference =================\n",
            "\n",
            "================= Loading vit_small_patch14_dinov2.lvd142m (5 folds) =================\n",
            "Model dir: /root/workspace/outputs/stage1_small\n",
            "Found 5 checkpoints\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Cannot find matching variant/backbone for tiled_film_best_model_fold0.pth. Detected tiled_film weights, please check training-inference consistency.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 578\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š Visualization saved: ensemble_analysis.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 578\u001b[0m     final_5, df_long, df_unique, final_a, final_b \u001b[38;5;241m=\u001b[39m \u001b[43mrun_dual_backbone_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     sub, wide_df \u001b[38;5;241m=\u001b[39m create_submission(final_5, df_long, df_unique)\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================= Generating Visualizations =================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[3], line 437\u001b[0m, in \u001b[0;36mrun_dual_backbone_ensemble\u001b[0;34m()\u001b[0m\n\u001b[1;32m    434\u001b[0m final_a \u001b[38;5;241m=\u001b[39m run_inference_for_backbone(CFG\u001b[38;5;241m.\u001b[39mMODEL_DIR_A, CFG\u001b[38;5;241m.\u001b[39mBACKBONE_A, test_unique, CFG\u001b[38;5;241m.\u001b[39mTEST_IMAGE_DIR)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================= Backbone B inference =================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 437\u001b[0m final_b \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference_for_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_DIR_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBACKBONE_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_unique\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTEST_IMAGE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m final_a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m final_b\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA/B results shape mismatch, cannot perform weighted fusion.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================= Fusing predictions (A:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG\u001b[38;5;241m.\u001b[39mW_A\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, B:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG\u001b[38;5;241m.\u001b[39mW_B\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) =================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[3], line 397\u001b[0m, in \u001b[0;36mrun_inference_for_backbone\u001b[0;34m(model_dir, backbone_name, test_unique, image_dir)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ckpt_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    396\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 397\u001b[0m m1, v1, b1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_fold_model_auto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGRID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackbone_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(m1)\n\u001b[1;32m    399\u001b[0m backbone_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(m1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_res\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m518\u001b[39m))\n",
            "Cell \u001b[0;32mIn[3], line 355\u001b[0m, in \u001b[0;36mload_fold_model_auto\u001b[0;34m(path, grid, backbone_hint)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find matching variant/backbone for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Detected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiled_film\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mis_film\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon-film\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, please check training-inference consistency.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot find matching variant/backbone for tiled_film_best_model_fold0.pth. Detected tiled_film weights, please check training-inference consistency."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    \"\"\"Configuration for top-2 backbone ensemble inference (all 5 folds each).\"\"\"\n",
        "    \n",
        "    # ==================== LOCAL TESTING MODE ====================\n",
        "    # Set LOCAL_TEST = True for local testing, False for Kaggle submission\n",
        "    LOCAL_TEST = True\n",
        "    \n",
        "    if LOCAL_TEST:\n",
        "        BASE_PATH = \"/root/workspace/data\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"train.csv\")  # Use train.csv for local OOF testing\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"train\")\n",
        "    else:\n",
        "        BASE_PATH = \"/kaggle/input/csiro-biomass\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"test\")\n",
        "    \n",
        "    # ==================== TOP 2 BACKBONE CONFIGURATIONS ====================\n",
        "    # Set paths to the 2 best backbones (selected by best avg CV RÂ² from training)\n",
        "    # Each directory should contain: tiled_film_best_model_fold{0-4}.pth\n",
        "    \n",
        "    # Backbone A: DINOv2 base reg4\n",
        "    MODEL_DIR_A = \"/root/workspace/outputs/stage1_base\"\n",
        "    BACKBONE_A = \"vit_base_patch14_reg4_dinov2.lvd142m\"  # timm model name\n",
        "    \n",
        "    # Backbone B: DINOv2 small reg4\n",
        "    MODEL_DIR_B = \"/root/workspace/outputs/stage1_small\"\n",
        "    BACKBONE_B = \"vit_small_patch14_reg4_dinov2.lvd142m\"  # timm model name\n",
        "    \n",
        "    # Ensemble weights (adjust based on CV scores - base got 0.8037)\n",
        "    W_A = 0.8\n",
        "    W_B = 0.2\n",
        "    \n",
        "    # ==================== INFERENCE SETTINGS ====================\n",
        "    SUBMISSION_FILE = \"submission.csv\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    BATCH_SIZE = 1\n",
        "    NUM_WORKERS = 0\n",
        "    \n",
        "    # Model architecture params (must match training)\n",
        "    DROPOUT = 0.30\n",
        "    HIDDEN_RATIO = 0.25\n",
        "    GRID = (2, 2)\n",
        "    \n",
        "    ALL_TARGET_COLS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
        "    \n",
        "    # DINO backbone candidates for auto-detection (fallback)\n",
        "    DINO_CANDIDATES = [\n",
        "        # DINOv2 (patch14) - with reg4\n",
        "        \"vit_large_patch14_reg4_dinov2.lvd142m\",\n",
        "        \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
        "        \"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
        "        # DINOv2 (patch14) - without reg4\n",
        "        \"vit_large_patch14_dinov2.lvd142m\",\n",
        "        \"vit_base_patch14_dinov2.lvd142m\",\n",
        "        \"vit_small_patch14_dinov2.lvd142m\",\n",
        "        # DINOv3 (patch16)\n",
        "        \"vit_small_patch16_dinov3\",\n",
        "        \"vit_base_patch16_dinov3\",\n",
        "        \"vit_large_patch16_dinov3\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_all_fold_checkpoints(model_dir: str, num_folds: int = 5) -> List[str]:\n",
        "    \"\"\"Get checkpoint paths for all folds of a backbone.\"\"\"\n",
        "    paths = []\n",
        "    for fold in range(num_folds):\n",
        "        ckpt_path = os.path.join(model_dir, f\"tiled_film_best_model_fold{fold}.pth\")\n",
        "        if os.path.exists(ckpt_path):\n",
        "            paths.append(ckpt_path)\n",
        "        else:\n",
        "            print(f\"  Warning: {ckpt_path} not found, skipping\")\n",
        "    return paths\n",
        "\n",
        "\n",
        "print(f\"Device: {CFG.DEVICE}\")\n",
        "print(f\"Backbone A: {CFG.BACKBONE_A} (weight={CFG.W_A})\")\n",
        "print(f\"Backbone B: {CFG.BACKBONE_B} (weight={CFG.W_B})\")\n",
        "\n",
        "class TestBiomassDataset(Dataset):\n",
        "    def __init__(self, df, transform, image_dir):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.image_dir = image_dir\n",
        "        self.paths = self.df[\"image_path\"].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = os.path.basename(self.paths[idx])\n",
        "        full_path = os.path.join(self.image_dir, filename)\n",
        "        img = cv2.imread(full_path)\n",
        "        if img is None:\n",
        "            img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        h, w, _ = img.shape\n",
        "        mid = w // 2\n",
        "        left = img[:, :mid]\n",
        "        right = img[:, mid:]\n",
        "\n",
        "        left_t = self.transform(image=left)[\"image\"]\n",
        "        right_t = self.transform(image=right)[\"image\"]\n",
        "        return left_t, right_t\n",
        "\n",
        "def _infer_input_res(m) -> int:\n",
        "    if hasattr(m, \"patch_embed\") and hasattr(m.patch_embed, \"img_size\"):\n",
        "        isz = m.patch_embed.img_size\n",
        "        return int(isz if isinstance(isz, (int, float)) else isz[0])\n",
        "    if hasattr(m, \"img_size\"):\n",
        "        isz = m.img_size\n",
        "        return int(isz if isinstance(isz, (int, float)) else isz[0])\n",
        "    dc = getattr(m, \"default_cfg\", {}) or {}\n",
        "    ins = dc.get(\"input_size\", None)\n",
        "    if ins:\n",
        "        if isinstance(ins, (tuple, list)) and len(ins) >= 2:\n",
        "            return int(ins[1])\n",
        "        return int(ins if isinstance(ins, (int, float)) else 224)\n",
        "    name = getattr(m, \"default_cfg\", {}).get(\"architecture\", \"\") or str(type(m))\n",
        "    # DINOv2: 518, DINOv3: 518 (both benefit from higher res)\n",
        "    if \"dinov2\" in name.lower() or \"dinov3\" in name.lower():\n",
        "        return 518\n",
        "    return 224\n",
        "\n",
        "def _build_dino_by_name(name: str):\n",
        "    m = timm.create_model(name, pretrained=False, num_classes=0)\n",
        "    feat = m.num_features\n",
        "    input_res = _infer_input_res(m)\n",
        "    return m, feat, input_res\n",
        "\n",
        "class TwoStreamDINOBase(nn.Module):\n",
        "    def __init__(self, backbone_name: str, dropout: float = 0.3, hidden_ratio: float = 0.25):\n",
        "        super().__init__()\n",
        "        self.backbone, feat, input_res = _build_dino_by_name(backbone_name)\n",
        "        self.used_backbone_name = backbone_name\n",
        "        self.input_res = int(input_res)\n",
        "        self.feat_dim = feat\n",
        "        self.combined = feat * 2\n",
        "\n",
        "        hidden = max(8, int(self.combined * hidden_ratio))\n",
        "\n",
        "        def head():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.combined, hidden),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden, 1),\n",
        "            )\n",
        "\n",
        "        self.head_green = head()\n",
        "        self.head_clover = head()\n",
        "        self.head_dead = head()\n",
        "        self.softplus = nn.Softplus(beta=1.0)\n",
        "\n",
        "    def _merge_heads(self, f_l: torch.Tensor, f_r: torch.Tensor):\n",
        "        \"\"\"Returns: green, dead, clover, gdm, total predictions.\"\"\"\n",
        "        f = torch.cat([f_l, f_r], dim=1)\n",
        "        green_pos = self.softplus(self.head_green(f))\n",
        "        clover_pos = self.softplus(self.head_clover(f))\n",
        "        dead_pos = self.softplus(self.head_dead(f))\n",
        "        gdm = green_pos + clover_pos\n",
        "        total = gdm + dead_pos\n",
        "        return green_pos, dead_pos, clover_pos, gdm, total\n",
        "\n",
        "class TwoStreamDINOPlain(TwoStreamDINOBase):\n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
        "        f_l = self.backbone(x_left)\n",
        "        f_r = self.backbone(x_right)\n",
        "        return self._merge_heads(f_l, f_r)\n",
        "\n",
        "def _make_edges(L: int, parts: int):\n",
        "    step = L // parts\n",
        "    edges = []\n",
        "    start = 0\n",
        "    for _ in range(parts - 1):\n",
        "        edges.append((start, start + step))\n",
        "        start += step\n",
        "    edges.append((start, L))\n",
        "    return edges\n",
        "\n",
        "class TwoStreamDINOTiled(TwoStreamDINOBase):\n",
        "    def __init__(self, backbone_name: str, grid=(2, 2), **kwargs):\n",
        "        super().__init__(backbone_name, **kwargs)\n",
        "        self.grid = tuple(grid)\n",
        "\n",
        "    def _encode_tiles(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, H, W = x.shape\n",
        "        r, c = self.grid\n",
        "        rows = _make_edges(H, r)\n",
        "        cols = _make_edges(W, c)\n",
        "        feats = []\n",
        "        for (rs, re) in rows:\n",
        "            for (cs, ce) in cols:\n",
        "                xt = x[:, :, rs:re, cs:ce]\n",
        "                if xt.shape[-2:] != (self.input_res, self.input_res):\n",
        "                    xt = F.interpolate(xt, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
        "                ft = self.backbone(xt)\n",
        "                feats.append(ft)\n",
        "        feats = torch.stack(feats, dim=0).permute(1, 0, 2)\n",
        "        feat_stream = feats.mean(dim=1)\n",
        "        return feat_stream\n",
        "\n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
        "        f_l = self._encode_tiles(x_left)\n",
        "        f_r = self._encode_tiles(x_right)\n",
        "        return self._merge_heads(f_l, f_r)\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, in_dim: int):\n",
        "        super().__init__()\n",
        "        hid = max(32, in_dim // 2)\n",
        "        self.mlp = nn.Sequential(nn.Linear(in_dim, hid), nn.ReLU(inplace=True), nn.Linear(hid, in_dim * 2))\n",
        "\n",
        "    def forward(self, context: torch.Tensor):\n",
        "        gb = self.mlp(context)\n",
        "        gamma, beta = torch.chunk(gb, 2, dim=1)\n",
        "        return gamma, beta\n",
        "\n",
        "class TwoStreamDINOTiledFiLM(TwoStreamDINOBase):\n",
        "    def __init__(self, backbone_name: str, grid=(2, 2), **kwargs):\n",
        "        super().__init__(backbone_name, **kwargs)\n",
        "        self.grid = tuple(grid)\n",
        "        self.film_left = FiLM(self.feat_dim)\n",
        "        self.film_right = FiLM(self.feat_dim)\n",
        "\n",
        "    def _tiles_backbone(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, H, W = x.shape\n",
        "        r, c = self.grid\n",
        "        rows = _make_edges(H, r)\n",
        "        cols = _make_edges(W, c)\n",
        "        feats = []\n",
        "        for (rs, re) in rows:\n",
        "            for (cs, ce) in cols:\n",
        "                xt = x[:, :, rs:re, cs:ce]\n",
        "                if xt.shape[-2:] != (self.input_res, self.input_res):\n",
        "                    xt = F.interpolate(xt, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
        "                ft = self.backbone(xt)\n",
        "                feats.append(ft)\n",
        "        feats = torch.stack(feats, dim=0).permute(1, 0, 2)\n",
        "        return feats\n",
        "\n",
        "    def _encode_stream(self, x: torch.Tensor, film: FiLM) -> torch.Tensor:\n",
        "        tiles = self._tiles_backbone(x)\n",
        "        context = tiles.mean(dim=1)\n",
        "        gamma, beta = film(context)\n",
        "        tiles = tiles * (1 + gamma.unsqueeze(1)) + beta.unsqueeze(1)\n",
        "        feat_stream = tiles.mean(dim=1)\n",
        "        return feat_stream\n",
        "\n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
        "        f_l = self._encode_stream(x_left, self.film_left)\n",
        "        f_r = self._encode_stream(x_right, self.film_right)\n",
        "        return self._merge_heads(f_l, f_r)\n",
        "\n",
        "def _strip_module_prefix(sd: dict):\n",
        "    if not sd:\n",
        "        return sd\n",
        "    keys = list(sd.keys())\n",
        "    if all(k.startswith(\"module.\") for k in keys):\n",
        "        return {k[len(\"module.\") :]: v for k, v in sd.items()}\n",
        "    return sd\n",
        "\n",
        "def _extract_student_substate(sd: dict) -> dict:\n",
        "    sd = _strip_module_prefix(sd)\n",
        "    has_student = any(k.startswith(\"student.\") for k in sd.keys())\n",
        "    if has_student:\n",
        "        sd = {k[len(\"student.\") :]: v for k, v in sd.items() if k.startswith(\"student.\")}\n",
        "    drop_prefixes = (\"txt_enc.\", \"img_proj.\", \"txt_film_left.\", \"txt_film_right.\")\n",
        "    sd = {k: v for k, v in sd.items() if not k.startswith(drop_prefixes)}\n",
        "    return sd\n",
        "\n",
        "def _has_film(sd_keys: set) -> bool:\n",
        "    return any(k.startswith(\"film_left.mlp.\") for k in sd_keys) or any(k.startswith(\"film_right.mlp.\") for k in sd_keys)\n",
        "\n",
        "def _try_build_and_load(sd: dict, backbone_name: str, variant: str, grid=(2, 2)):\n",
        "    if variant == \"tiled_film\":\n",
        "        model = TwoStreamDINOTiledFiLM(backbone_name, grid=grid, dropout=CFG.DROPOUT, hidden_ratio=CFG.HIDDEN_RATIO)\n",
        "    elif variant == \"tiled\":\n",
        "        model = TwoStreamDINOTiled(backbone_name, grid=grid, dropout=CFG.DROPOUT, hidden_ratio=CFG.HIDDEN_RATIO)\n",
        "    else:\n",
        "        model = TwoStreamDINOPlain(backbone_name, dropout=CFG.DROPOUT, hidden_ratio=CFG.HIDDEN_RATIO)\n",
        "\n",
        "    result = model.load_state_dict(sd, strict=False)\n",
        "    missing = getattr(result, \"missing_keys\", [])\n",
        "    unexpected = getattr(result, \"unexpected_keys\", [])\n",
        "    if len(missing) == 0 and len(unexpected) == 0:\n",
        "        model.to(CFG.DEVICE)\n",
        "        model.eval()\n",
        "        return model\n",
        "    return None\n",
        "\n",
        "def load_fold_model_auto(path: str, grid=(2, 2), backbone_hint: str = None):\n",
        "    \"\"\"\n",
        "    Load a fold model checkpoint.\n",
        "    \n",
        "    Args:\n",
        "        path: Path to the checkpoint file\n",
        "        grid: Tile grid configuration\n",
        "        backbone_hint: If provided, try this backbone first (speeds up loading)\n",
        "    \n",
        "    Returns:\n",
        "        (model, variant, backbone_name)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(path)\n",
        "    try:\n",
        "        raw_sd = torch.load(path, map_location=CFG.DEVICE, weights_only=True)\n",
        "    except TypeError:\n",
        "        raw_sd = torch.load(path, map_location=CFG.DEVICE)\n",
        "    sd = _extract_student_substate(raw_sd)\n",
        "    keys = set(sd.keys())\n",
        "\n",
        "    is_film = _has_film(keys)\n",
        "    variant_order = [\"tiled_film\"] if is_film else [\"tiled\", \"plain\"]\n",
        "    \n",
        "    # Build candidate list with hint first\n",
        "    candidates = list(CFG.DINO_CANDIDATES)\n",
        "    if backbone_hint and backbone_hint not in candidates:\n",
        "        candidates.insert(0, backbone_hint)\n",
        "    elif backbone_hint:\n",
        "        candidates.remove(backbone_hint)\n",
        "        candidates.insert(0, backbone_hint)\n",
        "\n",
        "    for variant in variant_order:\n",
        "        for backbone in candidates:\n",
        "            try:\n",
        "                m = _try_build_and_load(sd, backbone, variant, grid=grid)\n",
        "                if m is not None:\n",
        "                    return m, variant, backbone\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    raise RuntimeError(\n",
        "        f\"Cannot find matching variant/backbone for {os.path.basename(path)}.\"\n",
        "        f\" Detected {'tiled_film' if is_film else 'non-film'} weights, please check training-inference consistency.\"\n",
        "    )\n",
        "\n",
        "def get_tta_transforms(img_size: int):\n",
        "    base = [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]\n",
        "    original = A.Compose([A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base])\n",
        "    hflip = A.Compose([A.HorizontalFlip(p=1.0), A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base])\n",
        "    vflip = A.Compose([A.VerticalFlip(p=1.0), A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base])\n",
        "    return [original, hflip, vflip]\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one_view(models, loader):\n",
        "    out_list = []\n",
        "    amp_dtype = \"cuda\" if CFG.DEVICE.type == \"cuda\" else \"cpu\"\n",
        "\n",
        "    for (xl, xr) in tqdm(loader, desc=\"  Predicting View\", leave=False):\n",
        "        xl = xl.to(CFG.DEVICE, non_blocking=True)\n",
        "        xr = xr.to(CFG.DEVICE, non_blocking=True)\n",
        "\n",
        "        per_model_preds = []\n",
        "        with torch.amp.autocast(amp_dtype, enabled=(CFG.DEVICE.type == \"cuda\")):\n",
        "            for m in models:\n",
        "                green, dead, clover, gdm, total = m(xl, xr)\n",
        "                five = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
        "                five = torch.clamp(five, min=0.0)\n",
        "                per_model_preds.append(five.float().cpu())\n",
        "\n",
        "        stacked = torch.mean(torch.stack(per_model_preds, dim=0), dim=0)\n",
        "        out_list.append(stacked.numpy())\n",
        "\n",
        "    return np.concatenate(out_list, axis=0)\n",
        "\n",
        "def run_inference_for_backbone(model_dir: str, backbone_name: str, test_unique, image_dir):\n",
        "    \"\"\"Run inference for all 5 folds of a single backbone.\"\"\"\n",
        "    ckpt_list = get_all_fold_checkpoints(model_dir)\n",
        "    print(f\"\\n================= Loading {backbone_name} (5 folds) =================\")\n",
        "    print(f\"Model dir: {model_dir}\")\n",
        "    print(f\"Found {len(ckpt_list)} checkpoints\")\n",
        "    \n",
        "    models = []\n",
        "    m1, v1, b1 = load_fold_model_auto(ckpt_list[0], grid=CFG.GRID, backbone_hint=backbone_name)\n",
        "    models.append(m1)\n",
        "    backbone_res = int(getattr(m1, \"input_res\", 518))\n",
        "    print(f\"fold0 => variant={v1}, backbone={b1}, input_res={backbone_res}\")\n",
        "\n",
        "    for p in ckpt_list[1:]:\n",
        "        m, v, b = load_fold_model_auto(p, grid=CFG.GRID, backbone_hint=backbone_name)\n",
        "        print(f\"{os.path.basename(p)} => variant={v}, backbone={b}, input_res={getattr(m, 'input_res', '?')}\")\n",
        "        models.append(m)\n",
        "\n",
        "    tta_trans = get_tta_transforms(backbone_res)\n",
        "    per_view_preds = []\n",
        "    for i, t in enumerate(tta_trans):\n",
        "        print(f\"\\n--- TTA view {i+1}/{len(tta_trans)} (resize={backbone_res}) ---\")\n",
        "        ds = TestBiomassDataset(test_unique, t, image_dir)\n",
        "        dl = DataLoader(ds, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n",
        "        view_5 = predict_one_view(models, dl)\n",
        "        per_view_preds.append(view_5)\n",
        "\n",
        "    # Free GPU memory after each backbone\n",
        "    del models\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    final_5 = np.mean(per_view_preds, axis=0)\n",
        "    return final_5\n",
        "\n",
        "\n",
        "def run_dual_backbone_ensemble():\n",
        "    \"\"\"Run inference with top 2 backbones (5 folds each) and fuse predictions.\"\"\"\n",
        "    print(\"\\n================= Loading test data =================\")\n",
        "    test_long = pd.read_csv(CFG.TEST_CSV)\n",
        "    test_unique = test_long.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
        "    print(f\"Found {len(test_unique)} unique test images.\")\n",
        "\n",
        "    print(\"\\n================= Backbone A inference =================\")\n",
        "    final_a = run_inference_for_backbone(CFG.MODEL_DIR_A, CFG.BACKBONE_A, test_unique, CFG.TEST_IMAGE_DIR)\n",
        "\n",
        "    print(\"\\n================= Backbone B inference =================\")\n",
        "    final_b = run_inference_for_backbone(CFG.MODEL_DIR_B, CFG.BACKBONE_B, test_unique, CFG.TEST_IMAGE_DIR)\n",
        "\n",
        "    assert final_a.shape == final_b.shape, \"A/B results shape mismatch, cannot perform weighted fusion.\"\n",
        "    print(f\"\\n================= Fusing predictions (A:{CFG.W_A}, B:{CFG.W_B}) =================\")\n",
        "    final = CFG.W_A * final_a + CFG.W_B * final_b\n",
        "    return final, test_long, test_unique, final_a, final_b\n",
        "\n",
        "def create_submission(final_5, test_long, test_unique):\n",
        "    green = final_5[:, 0]\n",
        "    dead = final_5[:, 1]\n",
        "    clover = final_5[:, 2]\n",
        "    gdm = final_5[:, 3]\n",
        "    total = final_5[:, 4]\n",
        "\n",
        "    def nnz(x):\n",
        "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    green, dead, clover, gdm, total = map(nnz, [green, dead, clover, gdm, total])\n",
        "\n",
        "    wide = pd.DataFrame(\n",
        "        {\n",
        "            \"image_path\": test_unique[\"image_path\"],\n",
        "            \"Dry_Green_g\": green,\n",
        "            \"Dry_Dead_g\": dead,\n",
        "            \"Dry_Clover_g\": clover,\n",
        "            \"GDM_g\": gdm,\n",
        "            \"Dry_Total_g\": total,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    long_preds = wide.melt(\n",
        "        id_vars=[\"image_path\"],\n",
        "        value_vars=CFG.ALL_TARGET_COLS,\n",
        "        var_name=\"target_name\",\n",
        "        value_name=\"target\",\n",
        "    )\n",
        "\n",
        "    sub = pd.merge(\n",
        "        test_long[[\"sample_id\", \"image_path\", \"target_name\"]],\n",
        "        long_preds,\n",
        "        on=[\"image_path\", \"target_name\"],\n",
        "        how=\"left\",\n",
        "    )[[\"sample_id\", \"target\"]]\n",
        "\n",
        "    sub[\"target\"] = np.nan_to_num(sub[\"target\"], nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sub.to_csv(CFG.SUBMISSION_FILE, index=False)\n",
        "    print(f\"\\nðŸŽ‰ Generated submission file: {CFG.SUBMISSION_FILE}\")\n",
        "    print(sub.head())\n",
        "    return sub, wide\n",
        "\n",
        "def visualize_ensemble_analysis(final_a, final_b, final_fused, wide_df):\n",
        "    plt.style.use('dark_background')\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = GridSpec(3, 3, figure=fig, hspace=0.35, wspace=0.3)\n",
        "    \n",
        "    colors = ['#00d9ff', '#ff006e', '#8338ec', '#ffbe0b', '#06ffa5']\n",
        "    target_names = ['Green', 'Dead', 'Clover', 'GDM', 'Total']\n",
        "    \n",
        "    # Backbone names for labels\n",
        "    backbone_a_short = CFG.BACKBONE_A.replace(\"vit_\", \"\").replace(\"_dinov2.lvd142m\", \"\").replace(\"_dinov2\", \"\")\n",
        "    backbone_b_short = CFG.BACKBONE_B.replace(\"vit_\", \"\").replace(\"_dinov2.lvd142m\", \"\").replace(\"_dinov2\", \"\")\n",
        "    \n",
        "    ax1 = fig.add_subplot(gs[0, :])\n",
        "    ensemble_contributions = [CFG.W_A * 100, CFG.W_B * 100]\n",
        "    bars = ax1.barh([f'Backbone A ({backbone_a_short})', f'Backbone B ({backbone_b_short})'], \n",
        "                     ensemble_contributions, color=['#00d9ff', '#ff006e'], height=0.5, alpha=0.8)\n",
        "    ax1.set_xlabel('Contribution Weight (%)', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Top-2 Backbone Ensemble (5 folds each)', fontsize=16, fontweight='bold', pad=20)\n",
        "    ax1.set_xlim(0, 100)\n",
        "    ax1.grid(axis='x', alpha=0.2, linestyle='--')\n",
        "    for bar, val in zip(bars, ensemble_contributions):\n",
        "        ax1.text(val + 2, bar.get_y() + bar.get_height()/2, f'{val:.1f}%', \n",
        "                va='center', fontsize=11, fontweight='bold')\n",
        "    \n",
        "    ax2 = fig.add_subplot(gs[1, 0])\n",
        "    comparison_data = []\n",
        "    for i, name in enumerate(target_names):\n",
        "        comparison_data.extend([\n",
        "            {'Ensemble': 'A', 'Target': name, 'Mean': final_a[:, i].mean()},\n",
        "            {'Ensemble': 'B', 'Target': name, 'Mean': final_b[:, i].mean()},\n",
        "            {'Ensemble': 'Fused', 'Target': name, 'Mean': final_fused[:, i].mean()}\n",
        "        ])\n",
        "    comp_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    x = np.arange(len(target_names))\n",
        "    width = 0.25\n",
        "    for idx, ens in enumerate(['A', 'B', 'Fused']):\n",
        "        means = [comp_df[(comp_df['Ensemble'] == ens) & (comp_df['Target'] == t)]['Mean'].values[0] \n",
        "                for t in target_names]\n",
        "        ax2.bar(x + idx*width, means, width, label=f'Ensemble {ens}', \n",
        "               color=['#00d9ff', '#ff006e', '#8338ec'][idx], alpha=0.8)\n",
        "    ax2.set_xlabel('Biomass Component', fontsize=11, fontweight='bold')\n",
        "    ax2.set_ylabel('Mean Prediction (g)', fontsize=11, fontweight='bold')\n",
        "    ax2.set_title('Ensemble Comparison', fontsize=13, fontweight='bold')\n",
        "    ax2.set_xticks(x + width)\n",
        "    ax2.set_xticklabels(target_names, rotation=45, ha='right')\n",
        "    ax2.legend(framealpha=0.9)\n",
        "    ax2.grid(axis='y', alpha=0.2, linestyle='--')\n",
        "    \n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "    biomass_totals = wide_df[['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g']].sum()\n",
        "    wedges, texts, autotexts = ax3.pie(biomass_totals, labels=['Green', 'Dead', 'Clover'],\n",
        "                                         autopct='%1.1f%%', startangle=90,\n",
        "                                         colors=['#06ffa5', '#ff006e', '#ffbe0b'],\n",
        "                                         explode=(0.05, 0.05, 0.05),\n",
        "                                         textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
        "    ax3.set_title('Biomass Composition', fontsize=13, fontweight='bold')\n",
        "    \n",
        "    ax4 = fig.add_subplot(gs[1, 2])\n",
        "    disagreement = np.abs(final_a - final_b).mean(axis=0)\n",
        "    bars = ax4.bar(target_names, disagreement, color=colors, alpha=0.8)\n",
        "    ax4.set_xlabel('Target Variable', fontsize=11, fontweight='bold')\n",
        "    ax4.set_ylabel('Mean Absolute Difference (g)', fontsize=11, fontweight='bold')\n",
        "    ax4.set_title('Ensemble Disagreement', fontsize=13, fontweight='bold')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    ax4.grid(axis='y', alpha=0.2, linestyle='--')\n",
        "    for bar, val in zip(bars, disagreement):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, val + disagreement.max()*0.02, \n",
        "                f'{val:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "    \n",
        "    ax5 = fig.add_subplot(gs[2, :])\n",
        "    percentiles = [10, 25, 50, 75, 90]\n",
        "    for i, name in enumerate(target_names):\n",
        "        values = final_fused[:, i]\n",
        "        perc_values = np.percentile(values, percentiles)\n",
        "        ax5.plot(percentiles, perc_values, marker='o', linewidth=2.5, \n",
        "                label=name, color=colors[i], markersize=8, alpha=0.9)\n",
        "    ax5.set_xlabel('Percentile', fontsize=12, fontweight='bold')\n",
        "    ax5.set_ylabel('Biomass Prediction (g)', fontsize=12, fontweight='bold')\n",
        "    ax5.set_title('Prediction Distribution Analysis', fontsize=14, fontweight='bold')\n",
        "    ax5.legend(loc='best', framealpha=0.9, fontsize=10)\n",
        "    ax5.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax5.set_xticks(percentiles)\n",
        "    \n",
        "    plt.savefig('ensemble_analysis.png', dpi=150, bbox_inches='tight', \n",
        "                facecolor='#1a1a1a', edgecolor='none')\n",
        "    plt.show()\n",
        "    print(\"\\nðŸ“Š Visualization saved: ensemble_analysis.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    final_5, df_long, df_unique, final_a, final_b = run_dual_backbone_ensemble()\n",
        "    sub, wide_df = create_submission(final_5, df_long, df_unique)\n",
        "    \n",
        "    print(\"\\n================= Generating Visualizations =================\")\n",
        "    visualize_ensemble_analysis(final_a, final_b, final_5, wide_df)\n",
        "    \n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5859f0",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c08727",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 14254895,
          "isSourceIdPinned": false,
          "sourceId": 112509,
          "sourceType": "competition"
        },
        {
          "sourceId": 272104372,
          "sourceType": "kernelVersion"
        },
        {
          "isSourceIdPinned": false,
          "modelId": 487624,
          "modelInstanceId": 471723,
          "sourceId": 630489,
          "sourceType": "modelInstanceVersion"
        },
        {
          "isSourceIdPinned": false,
          "modelId": 487624,
          "modelInstanceId": 471723,
          "sourceId": 638209,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 126.613443,
      "end_time": "2025-11-26T12:41:58.238412",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-11-26T12:39:51.624969",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
