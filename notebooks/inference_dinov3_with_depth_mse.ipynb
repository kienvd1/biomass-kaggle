{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSIRO Image2Biomass – DINOv3 Direct Model Inference (with Depth)\n",
        "\n",
        "This notebook runs inference using DINOv3 Direct models trained with `dinov3_train.py`.\n",
        "\n",
        "**Model:**\n",
        "- `DINOv3Direct`: Predicts Total, Green, GDM directly; derives Dead, Clover\n",
        "- Components always sum to Total (mathematically guaranteed)\n",
        "\n",
        "**Depth Features (Depth Anything V2):**\n",
        "- `use_depth`: Extracts depth statistics (depth_gradient has r=0.63 correlation with green biomass!)\n",
        "- `use_depth_attention`: Uses depth maps to guide tile attention pooling\n",
        "\n",
        "**Other Features:**\n",
        "- Multi-fold ensemble (5-fold CV)\n",
        "- Test-Time Augmentation (TTA)\n",
        "- Vegetation Indices, Stereo Disparity (optional)\n",
        "- MPS/CUDA/CPU support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "============================================================\n",
            "DINOv3 Direct Model Inference\n",
            "============================================================\n",
            "Device: cuda\n",
            "Model dir: /workspace/biomass-kaggle/outputs/dinov3_full_mse\n",
            "TTA: default (3 views)\n",
            "Loaded config from results.json\n",
            "Model: DINOv3Direct\n",
            "Backbone: vit_base_patch16_dinov3\n",
            "Grid: (2, 2)\n",
            "Image size: 576\n",
            "Optional heads: train_dead=False, train_clover=True\n",
            "Innovative features: Depth Stats (DA2-small), Depth Attention (DA2-small), Presence Heads (Dead/Clover), NDVI Head, Height Head\n",
            "Using top-5 checkpoints for ensemble (train-all mode)\n",
            "Found 5 fold checkpoints\n",
            "\n",
            "Depth model path: /workspace/biomass-kaggle/outputs/dinov3_full_mse/depth_model\n",
            "\n",
            "Loading models...\n",
            "  Filtered 861 embedded depth model keys from checkpoint\n",
            "  Loaded: _topk_fold0_ep52.pth\n",
            "  Filtered 861 embedded depth model keys from checkpoint\n",
            "  Loaded: _topk_fold0_ep51.pth\n",
            "  Filtered 861 embedded depth model keys from checkpoint\n",
            "  Loaded: _topk_fold0_ep48.pth\n",
            "  Filtered 861 embedded depth model keys from checkpoint\n",
            "  Loaded: _topk_fold0_ep47.pth\n",
            "  Filtered 861 embedded depth model keys from checkpoint\n",
            "  Loaded: _topk_fold0_ep43.pth\n",
            "\n",
            "Loading test data...\n",
            "Test samples: 357\n",
            "\n",
            "Running inference...\n",
            "TTA level: default (3 views)\n",
            "  View 1/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  View 2/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  View 3/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions shape: (357, 5)\n",
            "\n",
            "Constraint check (G+D+C=T):\n",
            "  Max diff: 0.247450\n",
            "  Mean diff: 0.005716\n",
            "\n",
            "Prediction stats:\n",
            "  Green: mean=25.51, std=24.34, min=0.00, max=126.05\n",
            "  Dead: mean=11.00, std=9.35, min=0.00, max=42.67\n",
            "  Clover: mean=6.61, std=11.89, min=0.00, max=69.95\n",
            "  GDM: mean=32.13, std=24.36, min=0.62, max=128.31\n",
            "  Total: mean=43.12, std=26.37, min=0.62, max=139.16\n",
            "\n",
            "Creating submission...\n",
            "Saved: submission.csv\n",
            "                    sample_id         value\n",
            "0  ID1011485656__Dry_Clover_g  9.999725e-07\n",
            "1    ID1011485656__Dry_Dead_g  3.211934e+01\n",
            "2   ID1011485656__Dry_Green_g  1.832914e+01\n",
            "3   ID1011485656__Dry_Total_g  5.044847e+01\n",
            "4         ID1011485656__GDM_g  1.832914e+01\n",
            "5  ID1012260530__Dry_Clover_g  9.319995e-07\n",
            "6    ID1012260530__Dry_Dead_g  0.000000e+00\n",
            "7   ID1012260530__Dry_Green_g  8.601438e+00\n",
            "8   ID1012260530__Dry_Total_g  8.601438e+00\n",
            "9         ID1012260530__GDM_g  8.601438e+00\n",
            "                    sample_id         value\n",
            "0  ID1011485656__Dry_Clover_g  9.999725e-07\n",
            "1    ID1011485656__Dry_Dead_g  3.211934e+01\n",
            "2   ID1011485656__Dry_Green_g  1.832914e+01\n",
            "3   ID1011485656__Dry_Total_g  5.044847e+01\n",
            "4         ID1011485656__GDM_g  1.832914e+01\n",
            "5  ID1012260530__Dry_Clover_g  9.319995e-07\n",
            "6    ID1012260530__Dry_Dead_g  0.000000e+00\n",
            "7   ID1012260530__Dry_Green_g  8.601438e+00\n",
            "8   ID1012260530__Dry_Total_g  8.601438e+00\n",
            "9         ID1012260530__GDM_g  8.601438e+00\n",
            "\n",
            "============================================================\n",
            "Done!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# ==================== FIX PROTOBUF CONFLICT ON KAGGLE ====================\n",
        "# Kaggle has TensorFlow pre-installed which conflicts with transformers' protobuf\n",
        "# Use pure Python implementation to avoid the conflict\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "\n",
        "# ==================== KAGGLE OFFLINE MODE ====================\n",
        "# Set these BEFORE importing libraries that access HuggingFace\n",
        "# On Kaggle with no internet, this prevents network access attempts\n",
        "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
        "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "\n",
        "# Add parent directory to path for importing src modules\n",
        "sys.path.insert(0, os.path.dirname(os.path.abspath(\".\")))\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    \"\"\"Configuration for DINOv3 Direct model inference.\"\"\"\n",
        "    \n",
        "    # ==================== LOCAL TESTING MODE ====================\n",
        "    LOCAL_TEST = True\n",
        "    \n",
        "    if LOCAL_TEST:\n",
        "        BASE_PATH = \"../data\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"train.csv\")  # Use train.csv for local OOF testing\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"train\")\n",
        "        # Trained DINOv3 model directory (from dinov3_train.py)\n",
        "        MODEL_DIR = \"/workspace/biomass-kaggle/outputs/dinov3_full_mse\"  # Model with presence/ndvi/height heads\n",
        "    else:\n",
        "        BASE_PATH = \"/kaggle/input/csiro-biomass\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"test\")\n",
        "        MODEL_DIR = \"/kaggle/input/your-dinov3-model\"  # Update for Kaggle\n",
        "    \n",
        "    # DINOv3 backbone (fixed)\n",
        "    BACKBONE = \"vit_base_patch16_dinov3\"\n",
        "    \n",
        "    # ==================== INFERENCE SETTINGS ====================\n",
        "    SUBMISSION_FILE = \"submission.csv\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Updated below\n",
        "    BATCH_SIZE = 1\n",
        "    NUM_WORKERS = 0\n",
        "    \n",
        "    # Model architecture params (auto-loaded from results.json)\n",
        "    DROPOUT = 0.3\n",
        "    HIDDEN_RATIO = 0.25\n",
        "    GRID = (2, 2)\n",
        "    USE_FILM = True\n",
        "    USE_ATTENTION_POOL = True\n",
        "    TRAIN_DEAD = False  # Whether model has head_dead\n",
        "    TRAIN_CLOVER = False  # Whether model has head_clover\n",
        "    USE_VEGETATION_INDICES = False  # Whether model uses VI features\n",
        "    USE_DISPARITY = False  # Whether model uses stereo disparity features\n",
        "    USE_DEPTH = False  # Whether model uses Depth Anything V2 features\n",
        "    DEPTH_MODEL_SIZE = \"small\"  # Depth model size: \"small\" or \"base\"\n",
        "    USE_DEPTH_ATTENTION = False  # Whether model uses depth-guided attention\n",
        "    \n",
        "    # New auxiliary heads (presence, NDVI, height)\n",
        "    USE_PRESENCE_HEADS = False  # Whether model uses presence heads for Dead/Clover\n",
        "    USE_NDVI_HEAD = False  # Whether model uses NDVI regression head\n",
        "    USE_HEIGHT_HEAD = False  # Whether model uses height regression head\n",
        "    USE_SPECIES_HEAD = False  # Whether model uses species classification head\n",
        "    \n",
        "    # For Kaggle: local path to saved depth model (None = use HuggingFace)\n",
        "    # Save with: model.save_pretrained(\"path/to/depth_model\")\n",
        "    # Upload the folder as a Kaggle dataset\n",
        "    DEPTH_MODEL_PATH: Optional[str] = None  # e.g., \"/kaggle/input/depth-anything-v2/small\"\n",
        "    USE_LEARNABLE_AUG = False  # Whether model uses learnable augmentation\n",
        "    LEARNABLE_AUG_COLOR = True  # Learnable color augmentation\n",
        "    LEARNABLE_AUG_SPATIAL = False  # Learnable spatial augmentation\n",
        "    IMG_SIZE = 672  # DINOv3 default from dinov3_train.py\n",
        "    \n",
        "    # TTA settings\n",
        "    USE_TTA = True\n",
        "    TTA_LEVEL = \"default\"  # Options: \"none\", \"light\", \"default\", \"heavy\", \"extreme\"\n",
        "    # - none: No TTA (just base transform)\n",
        "    # - light: base + hflip (2 views)\n",
        "    # - default: base + hflip + brightness (3 views) \n",
        "    # - heavy: default + vflip + darker (5 views)\n",
        "    # - extreme: heavy + rotate90 + hue_shift + gamma (8 views)\n",
        "    \n",
        "    # Fold selection: None = use all folds, int = use top K folds by aR²\n",
        "    TOP_K_FOLDS: Optional[int] = None  # None = use all 5 folds\n",
        "    \n",
        "    ALL_TARGET_COLS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
        "\n",
        "\n",
        "# DINOv3 uses 256 native resolution but accepts any size divisible by 16\n",
        "DINOV3_NATIVE_RES = 256\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Get best available device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Update device\n",
        "CFG.DEVICE = get_device()\n",
        "\n",
        "print(f\"Device: {CFG.DEVICE}\")\n",
        "\n",
        "\n",
        "def load_config_from_results(model_dir: str) -> Dict:\n",
        "    \"\"\"Load training config from results.json if available.\"\"\"\n",
        "    results_path = os.path.join(model_dir, \"results.json\")\n",
        "    if os.path.exists(results_path):\n",
        "        with open(results_path) as f:\n",
        "            results = json.load(f)\n",
        "        return results.get(\"config\", {})\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ==================== DATASET ====================\n",
        "\n",
        "class TestBiomassDataset(Dataset):\n",
        "    \"\"\"Dataset for test/inference.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, image_dir: str, transform: A.Compose):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:\n",
        "        row = self.df.iloc[idx]\n",
        "        sample_id = row[\"sample_id_prefix\"]\n",
        "        \n",
        "        img_path = os.path.join(self.image_dir, f\"{sample_id}.jpg\")\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        H, W = img.shape[:2]\n",
        "        mid = W // 2\n",
        "        img_left = img[:, :mid, :]\n",
        "        img_right = img[:, mid:, :]\n",
        "        \n",
        "        if self.transform:\n",
        "            aug_left = self.transform(image=img_left)\n",
        "            aug_right = self.transform(image=img_right)\n",
        "            img_left = aug_left[\"image\"]\n",
        "            img_right = aug_right[\"image\"]\n",
        "        \n",
        "        return img_left, img_right, sample_id\n",
        "\n",
        "\n",
        "def get_val_transform(img_size: Optional[int] = None) -> A.Compose:\n",
        "    \"\"\"Validation transform matching dinov3_train.py exactly.\"\"\"\n",
        "    if img_size is None:\n",
        "        img_size = CFG.IMG_SIZE\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_tta_transforms(img_size: Optional[int] = None, level: str = \"default\") -> List[A.Compose]:\n",
        "    \"\"\"\n",
        "    TTA transforms with configurable levels.\n",
        "    \n",
        "    Args:\n",
        "        img_size: Image size (default: CFG.IMG_SIZE)\n",
        "        level: TTA level - \"none\", \"light\", \"default\", \"heavy\", \"extreme\"\n",
        "    \n",
        "    Returns:\n",
        "        List of albumentations Compose transforms\n",
        "    \"\"\"\n",
        "    if img_size is None:\n",
        "        img_size = CFG.IMG_SIZE\n",
        "    \n",
        "    # Base transform (always included)\n",
        "    base = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    if level == \"none\":\n",
        "        return [base]\n",
        "    \n",
        "    # Horizontal flip\n",
        "    hflip = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.HorizontalFlip(p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    if level == \"light\":\n",
        "        return [base, hflip]\n",
        "    \n",
        "    # Brightness/Contrast adjustment (slightly brighter)\n",
        "    bright = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.RandomBrightnessContrast(brightness_limit=(0.08, 0.12), contrast_limit=(0.08, 0.12), p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    if level == \"default\":\n",
        "        return [base, hflip, bright]\n",
        "    \n",
        "    # Vertical flip (vegetation is somewhat invariant to this)\n",
        "    vflip = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.VerticalFlip(p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    # Darker version (simulates different lighting)\n",
        "    darker = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.RandomBrightnessContrast(brightness_limit=(-0.12, -0.08), contrast_limit=(-0.05, 0.05), p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    if level == \"heavy\":\n",
        "        return [base, hflip, bright, vflip, darker]\n",
        "    \n",
        "    # 90° rotation\n",
        "    rotate90 = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.Rotate(limit=(90, 90), p=1.0, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    # Hue shift (simulates different camera settings)\n",
        "    hue_shift = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    # Gamma adjustment\n",
        "    gamma = A.Compose([\n",
        "        A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
        "        A.RandomGamma(gamma_limit=(90, 110), p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    if level == \"extreme\":\n",
        "        return [base, hflip, bright, vflip, darker, rotate90, hue_shift, gamma]\n",
        "    \n",
        "    # Default fallback\n",
        "    return [base, hflip, bright]\n",
        "\n",
        "\n",
        "# ==================== MODEL COMPONENTS ====================\n",
        "\n",
        "def build_dinov3_backbone(pretrained: bool = False) -> Tuple[nn.Module, int, int]:\n",
        "    \"\"\"Build DINOv3 backbone (vit_base_patch16_dinov3).\"\"\"\n",
        "    name = \"vit_base_patch16_dinov3\"\n",
        "    model = timm.create_model(name, pretrained=pretrained, num_classes=0)\n",
        "    feat_dim = model.num_features  # 768 for ViT-B\n",
        "    input_res = 256  # DINOv3 default\n",
        "    return model, feat_dim, input_res\n",
        "\n",
        "\n",
        "def _make_edges(length: int, n_parts: int) -> List[Tuple[int, int]]:\n",
        "    step = length // n_parts\n",
        "    edges = [(i * step, (i + 1) * step) for i in range(n_parts)]\n",
        "    edges[-1] = (edges[-1][0], length)\n",
        "    return edges\n",
        "\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, in_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        hidden = max(64, in_dim // 2)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, in_dim * 2),\n",
        "        )\n",
        "    \n",
        "    def forward(self, context: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        gb = self.mlp(context)\n",
        "        gamma, beta = torch.chunk(gb, 2, dim=1)\n",
        "        return gamma, beta\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.scale = dim ** -0.5\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        q = self.query(x.mean(dim=1, keepdim=True))\n",
        "        k = self.key(x)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        return (attn @ x).squeeze(1)\n",
        "\n",
        "\n",
        "# ==================== INNOVATIVE FEATURES ====================\n",
        "\n",
        "class VegetationIndices(nn.Module):\n",
        "    \"\"\"Compute vegetation indices (ExG, ExR, GRVI) from RGB image.\"\"\"\n",
        "    \n",
        "    def __init__(self, out_dim: int = 24) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(24, out_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1, 3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1, 3, 1, 1)\n",
        "        img_denorm = (img * std + mean).clamp(0, 1)\n",
        "        \n",
        "        r, g, b = img_denorm.unbind(dim=1)\n",
        "        exg = 2 * g - r - b\n",
        "        grvi = (g - r) / (g + r + 1e-6)\n",
        "        vari = (g - r) / (g + r - b + 1e-6)\n",
        "        exr = 1.4 * r - g\n",
        "        exgr = exg - exr\n",
        "        norm_g = g / (r + g + b + 1e-6)\n",
        "        \n",
        "        indices = torch.stack([exg, exr, exgr, grvi, norm_g, vari], dim=1)\n",
        "        feats = []\n",
        "        for i in range(indices.size(1)):\n",
        "            idx = indices[:, i]\n",
        "            feats.extend([\n",
        "                idx.mean(dim=(-2, -1)),\n",
        "                idx.std(dim=(-2, -1)),\n",
        "                idx.flatten(1).quantile(0.1, dim=1),\n",
        "                idx.flatten(1).quantile(0.9, dim=1),\n",
        "            ])\n",
        "        stats = torch.stack(feats, dim=1)\n",
        "        return self.proj(stats)\n",
        "\n",
        "\n",
        "class DisparityFeatures(nn.Module):\n",
        "    \"\"\"Extract stereo disparity features from tile features.\"\"\"\n",
        "    \n",
        "    def __init__(self, feat_dim: int, max_disparity: int = 8, out_dim: int = None) -> None:\n",
        "        super().__init__()\n",
        "        self.max_disparity = max_disparity\n",
        "        if out_dim is None:\n",
        "            out_dim = feat_dim // 4\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(max_disparity, out_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.diff_proj = nn.Sequential(\n",
        "            nn.Linear(6, out_dim // 2),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.out_dim = out_dim + out_dim // 2\n",
        "    \n",
        "    def forward(self, feat_left: torch.Tensor, feat_right: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, D = feat_left.shape\n",
        "        feat_l = F.normalize(feat_left, dim=-1)\n",
        "        feat_r = F.normalize(feat_right, dim=-1)\n",
        "        \n",
        "        correlations = []\n",
        "        for d in range(self.max_disparity):\n",
        "            shifted_r = torch.roll(feat_r, shifts=d, dims=1)\n",
        "            corr = (feat_l * shifted_r).sum(dim=-1).mean(dim=1)\n",
        "            correlations.append(corr)\n",
        "        corr_volume = torch.stack(correlations, dim=-1)\n",
        "        corr_feat = self.proj(corr_volume)\n",
        "        \n",
        "        fl_pooled = feat_left.mean(dim=1)\n",
        "        fr_pooled = feat_right.mean(dim=1)\n",
        "        fl_norm = F.normalize(fl_pooled, dim=-1)\n",
        "        fr_norm = F.normalize(fr_pooled, dim=-1)\n",
        "        \n",
        "        correlation = (fl_norm * fr_norm).sum(dim=-1, keepdim=True)\n",
        "        diff = fl_pooled - fr_pooled\n",
        "        diff_norm = diff.norm(dim=-1, keepdim=True)\n",
        "        diff_mean = diff.mean(dim=-1, keepdim=True)\n",
        "        diff_std = diff.std(dim=-1, keepdim=True)\n",
        "        ratio = fl_pooled / (fr_pooled + 1e-6)\n",
        "        ratio_mean = ratio.mean(dim=-1, keepdim=True)\n",
        "        ratio_std = ratio.std(dim=-1, keepdim=True)\n",
        "        \n",
        "        stats = torch.cat([correlation, diff_norm, diff_mean, diff_std, ratio_mean, ratio_std], dim=-1)\n",
        "        diff_feat = self.diff_proj(stats)\n",
        "        return torch.cat([corr_feat, diff_feat], dim=-1)\n",
        "\n",
        "\n",
        "class DepthFeatures(nn.Module):\n",
        "    \"\"\"\n",
        "    Extract depth-based features using Depth Anything V2.\n",
        "    \n",
        "    Uses frozen DA2 model to generate depth maps, then extracts statistics.\n",
        "    Key features: depth_gradient (r=0.63 with green), depth_mean, depth_range, depth_volume.\n",
        "    \n",
        "    For Kaggle submission: depth model is loaded upfront and weights come from checkpoint.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, out_dim: int = 32, model_size: str = \"small\", depth_model_path: Optional[str] = None) -> None:\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim\n",
        "        self.model_size = model_size\n",
        "        \n",
        "        # Load depth model upfront (weights will come from checkpoint)\n",
        "        from transformers import AutoModelForDepthEstimation\n",
        "        \n",
        "        # Use local path if provided (for Kaggle offline), else HuggingFace\n",
        "        if depth_model_path and os.path.exists(depth_model_path):\n",
        "            print(f\"    Loading depth model from local: {depth_model_path}\")\n",
        "            self._depth_model = AutoModelForDepthEstimation.from_pretrained(depth_model_path, local_files_only=True)\n",
        "        else:\n",
        "            model_names = {\n",
        "                \"small\": \"depth-anything/Depth-Anything-V2-Small-hf\",\n",
        "                \"base\": \"depth-anything/Depth-Anything-V2-Base-hf\",\n",
        "            }\n",
        "            model_name = model_names.get(model_size, model_names[\"small\"])\n",
        "            print(f\"    Loading depth model from HuggingFace: {model_name}\")\n",
        "            self._depth_model = AutoModelForDepthEstimation.from_pretrained(model_name)\n",
        "        \n",
        "        self._depth_model.eval()\n",
        "        \n",
        "        # Freeze depth model\n",
        "        for p in self._depth_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        # Project depth statistics to feature space\n",
        "        # 10 stats per view × 2 views + 2 stereo stats = 22 features\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(22, out_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(out_dim, out_dim),\n",
        "        )\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def _get_depth_map(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get depth map from image tensor.\"\"\"\n",
        "        device = img.device\n",
        "        \n",
        "        # Move depth model to same device if needed\n",
        "        if next(self._depth_model.parameters()).device != device:\n",
        "            self._depth_model = self._depth_model.to(device)\n",
        "        \n",
        "        # Denormalize from ImageNet to [0, 1]\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
        "        img_denorm = (img * std + mean).clamp(0, 1)\n",
        "        \n",
        "        # DA2 expects specific preprocessing - resize to 518\n",
        "        B, _, H, W = img.shape\n",
        "        img_resized = F.interpolate(img_denorm, size=(518, 518), mode=\"bilinear\", align_corners=False)\n",
        "        \n",
        "        # Apply DA2 normalization\n",
        "        da2_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
        "        da2_std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
        "        img_normalized = (img_resized - da2_mean) / da2_std\n",
        "        \n",
        "        # Get depth\n",
        "        outputs = self._depth_model(pixel_values=img_normalized)\n",
        "        depth = outputs.predicted_depth  # (B, h, w)\n",
        "        \n",
        "        # Resize back to original\n",
        "        depth = F.interpolate(\n",
        "            depth.unsqueeze(1), size=(H, W), mode=\"bilinear\", align_corners=False\n",
        "        ).squeeze(1)\n",
        "        \n",
        "        return depth\n",
        "    \n",
        "    def _compute_stats(self, depth: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute statistics from depth map.\"\"\"\n",
        "        B, H, W = depth.shape\n",
        "        flat = depth.view(B, -1)\n",
        "        \n",
        "        # Basic statistics\n",
        "        depth_mean = flat.mean(dim=1)\n",
        "        depth_std = flat.std(dim=1)\n",
        "        depth_min = flat.min(dim=1).values\n",
        "        depth_max = flat.max(dim=1).values\n",
        "        depth_range = depth_max - depth_min\n",
        "        \n",
        "        # Percentiles\n",
        "        depth_p10 = flat.quantile(0.1, dim=1)\n",
        "        depth_p90 = flat.quantile(0.9, dim=1)\n",
        "        \n",
        "        # Gradient (vegetation boundaries) - KEY FEATURE (r=0.63)\n",
        "        grad_y = torch.abs(depth[:, 1:, :] - depth[:, :-1, :]).mean(dim=(1, 2))\n",
        "        grad_x = torch.abs(depth[:, :, 1:] - depth[:, :, :-1]).mean(dim=(1, 2))\n",
        "        depth_gradient = grad_y + grad_x\n",
        "        \n",
        "        # Volume proxy (sum above minimum)\n",
        "        depth_volume = (flat - depth_min.unsqueeze(1)).mean(dim=1)\n",
        "        \n",
        "        # High depth ratio\n",
        "        threshold = flat.quantile(0.75, dim=1, keepdim=True)\n",
        "        depth_high_ratio = (flat > threshold).float().mean(dim=1)\n",
        "        \n",
        "        return torch.stack([\n",
        "            depth_mean, depth_std, depth_min, depth_max, depth_range,\n",
        "            depth_p10, depth_p90, depth_gradient, depth_volume, depth_high_ratio\n",
        "        ], dim=1)  # (B, 10)\n",
        "    \n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Extract depth features from stereo images.\"\"\"\n",
        "        # Get depth maps\n",
        "        depth_left = self._get_depth_map(x_left)\n",
        "        depth_right = self._get_depth_map(x_right)\n",
        "        \n",
        "        # Compute per-view statistics\n",
        "        stats_left = self._compute_stats(depth_left)    # (B, 10)\n",
        "        stats_right = self._compute_stats(depth_right)  # (B, 10)\n",
        "        \n",
        "        # Stereo statistics (L-R difference as disparity proxy)\n",
        "        depth_lr_diff = torch.abs(depth_left - depth_right).mean(dim=(1, 2)).unsqueeze(1)  # (B, 1)\n",
        "        depth_lr_corr = F.cosine_similarity(\n",
        "            depth_left.flatten(1), depth_right.flatten(1), dim=1\n",
        "        ).unsqueeze(1)  # (B, 1)\n",
        "        \n",
        "        # Combine all features\n",
        "        all_stats = torch.cat([\n",
        "            stats_left, stats_right, depth_lr_diff, depth_lr_corr\n",
        "        ], dim=1)  # (B, 22)\n",
        "        \n",
        "        return self.proj(all_stats)  # (B, out_dim)\n",
        "\n",
        "\n",
        "class DepthGuidedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Depth-guided attention for tile pooling.\n",
        "    Uses depth maps to weight which spatial regions contribute more to predictions.\n",
        "    \n",
        "    For Kaggle submission: depth model is loaded upfront and weights come from checkpoint.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        feat_dim: int, \n",
        "        grid: Tuple[int, int] = (2, 2),\n",
        "        model_size: str = \"small\",\n",
        "        depth_model_path: Optional[str] = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.feat_dim = feat_dim\n",
        "        self.grid = grid\n",
        "        self.num_tiles = grid[0] * grid[1]\n",
        "        \n",
        "        # Load depth model upfront (weights will come from checkpoint)\n",
        "        from transformers import AutoModelForDepthEstimation\n",
        "        \n",
        "        # Use local path if provided (for Kaggle offline), else HuggingFace\n",
        "        if depth_model_path and os.path.exists(depth_model_path):\n",
        "            print(f\"    Loading depth model from local: {depth_model_path}\")\n",
        "            self._depth_model = AutoModelForDepthEstimation.from_pretrained(depth_model_path, local_files_only=True)\n",
        "        else:\n",
        "            model_name = f\"depth-anything/Depth-Anything-V2-{model_size.capitalize()}-hf\"\n",
        "            print(f\"    Loading depth model from HuggingFace: {model_name}\")\n",
        "            self._depth_model = AutoModelForDepthEstimation.from_pretrained(model_name)\n",
        "        \n",
        "        self._depth_model.eval()\n",
        "        \n",
        "        # Freeze depth model\n",
        "        for p in self._depth_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        # Depth stats per tile (5 stats: mean, max, gradient, volume, high_ratio)\n",
        "        self.depth_stats_dim = 5\n",
        "        \n",
        "        # Depth → attention weight (per tile)\n",
        "        self.depth_to_attn = nn.Sequential(\n",
        "            nn.Linear(self.depth_stats_dim, 32),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "        \n",
        "        # Feature-based attention (like original)\n",
        "        self.query = nn.Linear(feat_dim, feat_dim)\n",
        "        self.key = nn.Linear(feat_dim, feat_dim)\n",
        "        self.scale = feat_dim ** -0.5\n",
        "        \n",
        "        # Combine depth attention and feature attention\n",
        "        self.gate = nn.Parameter(torch.tensor(0.5))\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def _get_depth_map(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get depth map from normalized image.\"\"\"\n",
        "        device = img.device\n",
        "        B, C, H, W = img.shape\n",
        "        \n",
        "        # Move depth model to same device if needed\n",
        "        if next(self._depth_model.parameters()).device != device:\n",
        "            self._depth_model = self._depth_model.to(device)\n",
        "        \n",
        "        # De-normalize from ImageNet\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
        "        img_denorm = (img * std + mean).clamp(0, 1)\n",
        "        \n",
        "        # DA2 expects specific preprocessing - resize to 518\n",
        "        img_resized = F.interpolate(img_denorm, size=(518, 518), mode=\"bilinear\", align_corners=False)\n",
        "        \n",
        "        # Apply DA2 normalization\n",
        "        da2_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
        "        da2_std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
        "        img_normalized = (img_resized - da2_mean) / da2_std\n",
        "        \n",
        "        # Get depth\n",
        "        outputs = self._depth_model(pixel_values=img_normalized)\n",
        "        depth = outputs.predicted_depth\n",
        "        \n",
        "        # Resize to match input\n",
        "        if depth.shape[-2:] != (H, W):\n",
        "            depth = F.interpolate(\n",
        "                depth.unsqueeze(1), size=(H, W), mode=\"bilinear\", align_corners=False\n",
        "            ).squeeze(1)\n",
        "        \n",
        "        return depth\n",
        "    \n",
        "    def _compute_tile_stats(self, depth: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute per-tile depth statistics.\"\"\"\n",
        "        B, H, W = depth.shape\n",
        "        r, c = self.grid\n",
        "        \n",
        "        tile_h = H // r\n",
        "        tile_w = W // c\n",
        "        \n",
        "        stats_list = []\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                tile = depth[:, i*tile_h:(i+1)*tile_h, j*tile_w:(j+1)*tile_w]\n",
        "                flat = tile.reshape(B, -1)\n",
        "                \n",
        "                tile_mean = flat.mean(dim=1)\n",
        "                tile_max = flat.max(dim=1).values\n",
        "                \n",
        "                # Gradient\n",
        "                grad_y = torch.abs(tile[:, 1:, :] - tile[:, :-1, :]).mean(dim=(1, 2))\n",
        "                grad_x = torch.abs(tile[:, :, 1:] - tile[:, :, :-1]).mean(dim=(1, 2))\n",
        "                tile_gradient = grad_y + grad_x\n",
        "                \n",
        "                # Volume\n",
        "                tile_min = flat.min(dim=1).values\n",
        "                tile_volume = (flat - tile_min.unsqueeze(1)).mean(dim=1)\n",
        "                \n",
        "                # High ratio\n",
        "                threshold = flat.quantile(0.75, dim=1, keepdim=True)\n",
        "                tile_high_ratio = (flat > threshold).float().mean(dim=1)\n",
        "                \n",
        "                stats = torch.stack([tile_mean, tile_max, tile_gradient, tile_volume, tile_high_ratio], dim=1)\n",
        "                stats_list.append(stats)\n",
        "        \n",
        "        return torch.stack(stats_list, dim=1)  # (B, num_tiles, 5)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, img: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, num_tiles, D) tile features\n",
        "            img: (B, 3, H, W) original image for depth\n",
        "        Returns:\n",
        "            pooled: (B, D)\n",
        "        \"\"\"\n",
        "        B, N, D = x.shape\n",
        "        \n",
        "        # Get depth-based attention weights\n",
        "        depth_map = self._get_depth_map(img)  # (B, H, W)\n",
        "        tile_stats = self._compute_tile_stats(depth_map)  # (B, num_tiles, 5)\n",
        "        depth_attn = self.depth_to_attn(tile_stats).squeeze(-1)  # (B, num_tiles)\n",
        "        depth_attn = F.softmax(depth_attn, dim=-1)\n",
        "        \n",
        "        # Get feature-based attention weights\n",
        "        q = self.query(x.mean(dim=1, keepdim=True))  # (B, 1, D)\n",
        "        k = self.key(x)  # (B, N, D)\n",
        "        feat_attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, 1, N)\n",
        "        feat_attn = F.softmax(feat_attn, dim=-1).squeeze(1)  # (B, N)\n",
        "        \n",
        "        # Combine attention weights\n",
        "        gate = torch.sigmoid(self.gate)\n",
        "        combined_attn = gate * depth_attn + (1 - gate) * feat_attn\n",
        "        combined_attn = combined_attn / combined_attn.sum(dim=-1, keepdim=True)\n",
        "        \n",
        "        # Apply attention\n",
        "        pooled = (combined_attn.unsqueeze(-1) * x).sum(dim=1)  # (B, D)\n",
        "        \n",
        "        return pooled\n",
        "\n",
        "\n",
        "class LearnableAugmentation(nn.Module):\n",
        "    \"\"\"Learnable augmentation module (identity at inference time).\n",
        "    \n",
        "    Includes all strong augmentations: color, spatial, blur, CLAHE.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        enable_color: bool = True,\n",
        "        enable_spatial: bool = False,\n",
        "        enable_blur: bool = True,\n",
        "        enable_local_contrast: bool = True,\n",
        "        color_strength: float = 0.25,\n",
        "        spatial_strength: float = 0.15,\n",
        "        noise_std: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.enable_color = enable_color\n",
        "        self.enable_spatial = enable_spatial\n",
        "        self.enable_blur = enable_blur\n",
        "        self.enable_local_contrast = enable_local_contrast\n",
        "        \n",
        "        if enable_color:\n",
        "            self.color_params = nn.Parameter(torch.zeros(6))\n",
        "            self.color_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 32),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(32, 6),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "        \n",
        "        if enable_spatial:\n",
        "            self.spatial_params = nn.Parameter(torch.zeros(5))\n",
        "            self.spatial_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 32),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(32, 5),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "        \n",
        "        if enable_blur:\n",
        "            self.blur_params = nn.Parameter(torch.zeros(2))\n",
        "            self.blur_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 16),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(16, 2),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "            self.blur_kernel = nn.Parameter(torch.tensor([\n",
        "                [1., 2., 1.],\n",
        "                [2., 4., 2.],\n",
        "                [1., 2., 1.],\n",
        "            ]) / 16.0)\n",
        "        \n",
        "        if enable_local_contrast:\n",
        "            self.contrast_params = nn.Parameter(torch.zeros(2))\n",
        "            self.contrast_predictor = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(3, 16),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(16, 2),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "    \n",
        "    def forward(self, img: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Identity during inference (eval mode)\n",
        "        return img, torch.tensor(0.0, device=img.device)\n",
        "\n",
        "\n",
        "# ==================== DINOv3 DIRECT MODEL ====================\n",
        "\n",
        "class DINOv3Direct(nn.Module):\n",
        "    \"\"\"\n",
        "    DINOv3 Direct Model for Biomass Prediction.\n",
        "    Predicts Total, Green, GDM directly; optionally predicts Dead, Clover.\n",
        "    Components always sum to Total (mathematically guaranteed).\n",
        "    \n",
        "    Optional features:\n",
        "    - Vegetation Indices (VI): ExG, ExR, GRVI etc.\n",
        "    - Stereo Disparity: 3D volume features from stereo correspondence\n",
        "    - Depth Features: Depth Anything V2 depth maps (r=0.63 correlation with green!)\n",
        "    - Depth-guided Attention: Uses depth to weight tile attention\n",
        "    \"\"\"\n",
        "    \n",
        "    # Auxiliary head class counts (must match dataset.py)\n",
        "    NUM_STATES = 4\n",
        "    NUM_MONTHS = 10\n",
        "    NUM_SPECIES = 8\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        grid: Tuple[int, int] = (2, 2),\n",
        "        pretrained: bool = False,\n",
        "        dropout: float = 0.3,\n",
        "        hidden_ratio: float = 0.25,\n",
        "        use_film: bool = True,\n",
        "        use_attention_pool: bool = True,\n",
        "        train_dead: bool = False,\n",
        "        train_clover: bool = False,\n",
        "        use_vegetation_indices: bool = False,\n",
        "        use_disparity: bool = False,\n",
        "        use_depth: bool = False,\n",
        "        depth_model_size: str = \"small\",\n",
        "        use_depth_attention: bool = False,\n",
        "        use_learnable_aug: bool = False,\n",
        "        learnable_aug_color: bool = True,\n",
        "        learnable_aug_spatial: bool = False,\n",
        "        depth_model_path: Optional[str] = None,  # For Kaggle offline: local path to depth model\n",
        "        use_presence_heads: bool = False,  # Binary presence for Dead/Clover\n",
        "        use_ndvi_head: bool = False,  # NDVI auxiliary head\n",
        "        use_height_head: bool = False,  # Height auxiliary head\n",
        "        use_species_head: bool = False,  # Species classification head\n",
        "        ckpt_path: Optional[str] = None,  # For custom backbone weights (ignored if pretrained=False)\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        # Build DINOv3 backbone\n",
        "        self.backbone, self.feat_dim, self.input_res = build_dinov3_backbone(pretrained)\n",
        "        self.grid = tuple(grid)\n",
        "        self.use_film = use_film\n",
        "        self.use_attention_pool = use_attention_pool\n",
        "        self.train_dead = train_dead\n",
        "        self.train_clover = train_clover\n",
        "        self.use_vegetation_indices = use_vegetation_indices\n",
        "        self.use_disparity = use_disparity\n",
        "        self.use_depth = use_depth\n",
        "        self.use_depth_attention = use_depth_attention\n",
        "        self.use_learnable_aug = use_learnable_aug\n",
        "        self.depth_model_size = depth_model_size\n",
        "        self.use_presence_heads = use_presence_heads\n",
        "        self.use_ndvi_head = use_ndvi_head\n",
        "        self.use_height_head = use_height_head\n",
        "        self.use_species_head = use_species_head\n",
        "        self.hidden_dim = max(64, int((self.feat_dim * 2) * hidden_ratio))\n",
        "        \n",
        "        # Learnable augmentation (identity at inference)\n",
        "        # Includes all strong augmentations: color, spatial, blur, CLAHE\n",
        "        if use_learnable_aug:\n",
        "            self.learnable_aug_left = LearnableAugmentation(\n",
        "                enable_color=learnable_aug_color,\n",
        "                enable_spatial=learnable_aug_spatial,\n",
        "                enable_blur=learnable_aug_color,\n",
        "                enable_local_contrast=learnable_aug_color,\n",
        "            )\n",
        "            self.learnable_aug_right = LearnableAugmentation(\n",
        "                enable_color=learnable_aug_color,\n",
        "                enable_spatial=learnable_aug_spatial,\n",
        "                enable_blur=learnable_aug_color,\n",
        "                enable_local_contrast=learnable_aug_color,\n",
        "            )\n",
        "        \n",
        "        # FiLM for cross-view conditioning\n",
        "        if use_film:\n",
        "            self.film_left = FiLM(self.feat_dim)\n",
        "            self.film_right = FiLM(self.feat_dim)\n",
        "        \n",
        "        # Attention pooling for tiles (or depth-guided attention)\n",
        "        if use_depth_attention:\n",
        "            self.attn_pool_left = DepthGuidedAttention(self.feat_dim, grid=grid, model_size=depth_model_size, depth_model_path=depth_model_path)\n",
        "            self.attn_pool_right = DepthGuidedAttention(self.feat_dim, grid=grid, model_size=depth_model_size, depth_model_path=depth_model_path)\n",
        "        elif use_attention_pool:\n",
        "            self.attn_pool_left = AttentionPooling(self.feat_dim)\n",
        "            self.attn_pool_right = AttentionPooling(self.feat_dim)\n",
        "        \n",
        "        # === Optional feature modules ===\n",
        "        extra_dim = 0\n",
        "        if use_vegetation_indices:\n",
        "            vi_out = 24\n",
        "            self.vi_left = VegetationIndices(out_dim=vi_out)\n",
        "            self.vi_right = VegetationIndices(out_dim=vi_out)\n",
        "            extra_dim += vi_out * 2\n",
        "        \n",
        "        if use_disparity:\n",
        "            self.disparity_module = DisparityFeatures(self.feat_dim, max_disparity=8, out_dim=self.feat_dim // 4)\n",
        "            extra_dim += self.disparity_module.out_dim\n",
        "        \n",
        "        # Depth Features (Depth Anything V2)\n",
        "        if use_depth:\n",
        "            depth_out_dim = 32\n",
        "            self.depth_module = DepthFeatures(out_dim=depth_out_dim, model_size=depth_model_size, depth_model_path=depth_model_path)\n",
        "            extra_dim += depth_out_dim\n",
        "        \n",
        "        # Head dimensions\n",
        "        combined_dim = self.feat_dim * 2 + extra_dim\n",
        "        hidden_dim = max(64, int((self.feat_dim * 2) * hidden_ratio))\n",
        "        \n",
        "        # Shared projection\n",
        "        self.shared_proj = nn.Sequential(\n",
        "            nn.LayerNorm(combined_dim),\n",
        "            nn.Linear(combined_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "        # Prediction heads\n",
        "        def _make_head() -> nn.Sequential:\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout * 0.5),\n",
        "                nn.Linear(hidden_dim, 1),\n",
        "            )\n",
        "        \n",
        "        self.head_total = _make_head()\n",
        "        self.head_green = _make_head()\n",
        "        self.head_gdm = _make_head()\n",
        "        \n",
        "        # Optional heads for Dead and Clover\n",
        "        self.head_dead = _make_head() if train_dead else None\n",
        "        self.head_clover = _make_head() if train_clover else None\n",
        "        \n",
        "        # Presence heads: Binary classification for \"has Dead?\" / \"has Clover?\"\n",
        "        if use_presence_heads:\n",
        "            self.head_dead_presence = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "            )\n",
        "            self.head_clover_presence = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "            )\n",
        "        \n",
        "        # NDVI auxiliary head\n",
        "        if use_ndvi_head:\n",
        "            self.head_ndvi = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "        \n",
        "        # Height auxiliary head\n",
        "        if use_height_head:\n",
        "            self.head_height = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "                nn.Softplus(),\n",
        "            )\n",
        "        \n",
        "        # Species classification head\n",
        "        if use_species_head:\n",
        "            self.head_species_only = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim // 2, self.NUM_SPECIES),\n",
        "            )\n",
        "        \n",
        "        self.softplus = nn.Softplus(beta=1.0)\n",
        "    \n",
        "    def _collect_tiles(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        \"\"\"Split image into grid of tiles.\"\"\"\n",
        "        _, _, H, W = x.shape\n",
        "        r, c = self.grid\n",
        "        rows = _make_edges(H, r)\n",
        "        cols = _make_edges(W, c)\n",
        "        \n",
        "        tiles = []\n",
        "        for rs, re in rows:\n",
        "            for cs, ce in cols:\n",
        "                tile = x[:, :, rs:re, cs:ce]\n",
        "                if tile.shape[-2:] != (self.input_res, self.input_res):\n",
        "                    tile = F.interpolate(\n",
        "                        tile,\n",
        "                        size=(self.input_res, self.input_res),\n",
        "                        mode=\"bilinear\",\n",
        "                        align_corners=False,\n",
        "                    )\n",
        "                tiles.append(tile)\n",
        "        return tiles\n",
        "    \n",
        "    def _extract_features(\n",
        "        self, x_left: torch.Tensor, x_right: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract tile features from both views in one backbone call.\"\"\"\n",
        "        B = x_left.size(0)\n",
        "        \n",
        "        tiles_left = self._collect_tiles(x_left)\n",
        "        tiles_right = self._collect_tiles(x_right)\n",
        "        num_tiles = len(tiles_left)\n",
        "        \n",
        "        # Process all tiles in one forward pass\n",
        "        all_tiles = torch.cat(tiles_left + tiles_right, dim=0)\n",
        "        all_feats = self.backbone(all_tiles)\n",
        "        \n",
        "        # Reshape\n",
        "        all_feats = all_feats.view(2 * num_tiles, B, -1).permute(1, 0, 2)\n",
        "        feats_left = all_feats[:, :num_tiles, :]\n",
        "        feats_right = all_feats[:, num_tiles:, :]\n",
        "        \n",
        "        return feats_left, feats_right\n",
        "    \n",
        "    def forward(\n",
        "        self, x_left: torch.Tensor, x_right: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        # Extract tile features\n",
        "        tiles_left, tiles_right = self._extract_features(x_left, x_right)\n",
        "        \n",
        "        # Stereo Disparity Features (before FiLM)\n",
        "        disp_feat = None\n",
        "        if self.use_disparity:\n",
        "            disp_feat = self.disparity_module(tiles_left, tiles_right)\n",
        "        \n",
        "        # Context for FiLM\n",
        "        ctx_left = tiles_left.mean(dim=1)\n",
        "        ctx_right = tiles_right.mean(dim=1)\n",
        "        \n",
        "        # Apply FiLM cross-conditioning\n",
        "        if self.use_film:\n",
        "            gamma_l, beta_l = self.film_left(ctx_right)\n",
        "            gamma_r, beta_r = self.film_right(ctx_left)\n",
        "            tiles_left = tiles_left * (1 + gamma_l.unsqueeze(1)) + beta_l.unsqueeze(1)\n",
        "            tiles_right = tiles_right * (1 + gamma_r.unsqueeze(1)) + beta_r.unsqueeze(1)\n",
        "        \n",
        "        # Pool tiles\n",
        "        if self.use_depth_attention:\n",
        "            # Depth-guided attention needs the original images\n",
        "            f_left = self.attn_pool_left(tiles_left, x_left)\n",
        "            f_right = self.attn_pool_right(tiles_right, x_right)\n",
        "        elif self.use_attention_pool:\n",
        "            f_left = self.attn_pool_left(tiles_left)\n",
        "            f_right = self.attn_pool_right(tiles_right)\n",
        "        else:\n",
        "            f_left = tiles_left.mean(dim=1)\n",
        "            f_right = tiles_right.mean(dim=1)\n",
        "        \n",
        "        # Combine features\n",
        "        features_list = [f_left, f_right]\n",
        "        \n",
        "        if self.use_vegetation_indices:\n",
        "            vi_left = self.vi_left(x_left)\n",
        "            vi_right = self.vi_right(x_right)\n",
        "            features_list.extend([vi_left, vi_right])\n",
        "        \n",
        "        if disp_feat is not None:\n",
        "            features_list.append(disp_feat)\n",
        "        \n",
        "        # Depth Features (Depth Anything V2)\n",
        "        if self.use_depth:\n",
        "            depth_feat = self.depth_module(x_left, x_right)\n",
        "            features_list.append(depth_feat)\n",
        "        \n",
        "        f = torch.cat(features_list, dim=1)\n",
        "        f = self.shared_proj(f)\n",
        "        \n",
        "        # Core predictions\n",
        "        total_raw = self.softplus(self.head_total(f))\n",
        "        green_raw = self.softplus(self.head_green(f))\n",
        "        gdm_raw = self.softplus(self.head_gdm(f))\n",
        "        \n",
        "        # Enforce constraints: Total >= GDM >= Green\n",
        "        total = total_raw\n",
        "        gdm = torch.minimum(gdm_raw, total)\n",
        "        green = torch.minimum(green_raw, gdm)\n",
        "        \n",
        "        # Presence probabilities for gating (if enabled)\n",
        "        dead_presence_logit = None\n",
        "        clover_presence_logit = None\n",
        "        if self.use_presence_heads:\n",
        "            dead_presence_logit = self.head_dead_presence(f)\n",
        "            clover_presence_logit = self.head_clover_presence(f)\n",
        "            dead_presence_prob = torch.sigmoid(dead_presence_logit)\n",
        "            clover_presence_prob = torch.sigmoid(clover_presence_logit)\n",
        "        \n",
        "        # Dead: predicted or derived\n",
        "        if self.head_dead is not None:\n",
        "            dead_raw = self.softplus(self.head_dead(f))\n",
        "            dead = torch.minimum(dead_raw, total - gdm + 1e-6)\n",
        "            dead = F.relu(dead)\n",
        "            if self.use_presence_heads:\n",
        "                dead = dead * dead_presence_prob\n",
        "        else:\n",
        "            dead = F.relu(total - gdm)\n",
        "        \n",
        "        # Clover: predicted or derived\n",
        "        if self.head_clover is not None:\n",
        "            clover_raw = self.softplus(self.head_clover(f))\n",
        "            clover = torch.minimum(clover_raw, gdm - green + 1e-6)\n",
        "            clover = F.relu(clover)\n",
        "            if self.use_presence_heads:\n",
        "                clover = clover * clover_presence_prob\n",
        "        else:\n",
        "            clover = F.relu(gdm - green)\n",
        "        \n",
        "        # Auxiliary predictions\n",
        "        ndvi_pred = None\n",
        "        if self.use_ndvi_head:\n",
        "            ndvi_pred = self.head_ndvi(f)\n",
        "        \n",
        "        height_pred = None\n",
        "        if self.use_height_head:\n",
        "            height_pred = self.head_height(f)\n",
        "        \n",
        "        species_logits = None\n",
        "        if self.use_species_head:\n",
        "            species_logits = self.head_species_only(f)\n",
        "        \n",
        "        # Return with auxiliary outputs if any enabled\n",
        "        aux_loss = torch.tensor(0.0, device=x_left.device)\n",
        "        if self.use_presence_heads or self.use_ndvi_head or self.use_height_head or self.use_species_head:\n",
        "            return green, dead, clover, gdm, total, aux_loss, dead_presence_logit, clover_presence_logit, ndvi_pred, height_pred, species_logits\n",
        "        \n",
        "        return green, dead, clover, gdm, total, aux_loss\n",
        "\n",
        "\n",
        "# ==================== MODEL LOADING ====================\n",
        "\n",
        "def _strip_module_prefix(sd: dict) -> dict:\n",
        "    \"\"\"Remove 'module.' prefix from state dict keys (for DDP-trained models).\"\"\"\n",
        "    if not sd:\n",
        "        return sd\n",
        "    keys = list(sd.keys())\n",
        "    if all(k.startswith(\"module.\") for k in keys):\n",
        "        return {k[len(\"module.\"):]: v for k, v in sd.items()}\n",
        "    return sd\n",
        "\n",
        "\n",
        "def _filter_depth_model_keys(sd: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Filter out embedded depth model weights from old checkpoints.\n",
        "    \n",
        "    Old checkpoints saved depth model weights as part of state_dict.\n",
        "    New code loads depth model separately, so these keys are unexpected.\n",
        "    \"\"\"\n",
        "    if not sd:\n",
        "        return sd\n",
        "    \n",
        "    filtered = {}\n",
        "    removed_count = 0\n",
        "    for k, v in sd.items():\n",
        "        if \"._depth_model.\" in k:\n",
        "            removed_count += 1\n",
        "        else:\n",
        "            filtered[k] = v\n",
        "    \n",
        "    if removed_count > 0:\n",
        "        print(f\"  Filtered {removed_count} embedded depth model keys from checkpoint\")\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "\n",
        "def _detect_model_config(sd_keys: set) -> dict:\n",
        "    \"\"\"Auto-detect model config from checkpoint keys.\"\"\"\n",
        "    # Detect learnable aug color/spatial from keys\n",
        "    has_learnable_aug = any(k.startswith(\"learnable_aug_left.\") for k in sd_keys)\n",
        "    learnable_aug_color = any(\"color_params\" in k for k in sd_keys if k.startswith(\"learnable_aug_left.\"))\n",
        "    learnable_aug_spatial = any(\"spatial_params\" in k for k in sd_keys if k.startswith(\"learnable_aug_left.\"))\n",
        "    \n",
        "    # Detect depth-guided attention (has depth_to_attn in attn_pool)\n",
        "    has_depth_attention = any(\"depth_to_attn\" in k for k in sd_keys if k.startswith(\"attn_pool_left.\"))\n",
        "    \n",
        "    # Detect new auxiliary heads\n",
        "    has_presence_heads = any(k.startswith(\"head_dead_presence.\") for k in sd_keys)\n",
        "    has_ndvi_head = any(k.startswith(\"head_ndvi.\") for k in sd_keys)\n",
        "    has_height_head = any(k.startswith(\"head_height.\") for k in sd_keys)\n",
        "    has_species_head = any(k.startswith(\"head_species_only.\") for k in sd_keys)\n",
        "    \n",
        "    return {\n",
        "        \"use_film\": any(k.startswith(\"film_left.\") for k in sd_keys),\n",
        "        \"use_attention_pool\": any(k.startswith(\"attn_pool_left.\") for k in sd_keys),\n",
        "        \"train_dead\": any(k.startswith(\"head_dead.\") for k in sd_keys),\n",
        "        \"train_clover\": any(k.startswith(\"head_clover.\") for k in sd_keys),\n",
        "        \"use_vegetation_indices\": any(k.startswith(\"vi_left.\") for k in sd_keys),\n",
        "        \"use_disparity\": any(k.startswith(\"disparity_module.\") for k in sd_keys),\n",
        "        \"use_depth\": any(k.startswith(\"depth_module.\") for k in sd_keys),\n",
        "        \"use_depth_attention\": has_depth_attention,\n",
        "        \"use_learnable_aug\": has_learnable_aug,\n",
        "        \"learnable_aug_color\": learnable_aug_color,\n",
        "        \"learnable_aug_spatial\": learnable_aug_spatial,\n",
        "        \"use_presence_heads\": has_presence_heads,\n",
        "        \"use_ndvi_head\": has_ndvi_head,\n",
        "        \"use_height_head\": has_height_head,\n",
        "        \"use_species_head\": has_species_head,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_depth_model_path() -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Get the path to the depth model for inference.\n",
        "    \n",
        "    Priority:\n",
        "    1. CFG.DEPTH_MODEL_PATH if explicitly set\n",
        "    2. MODEL_DIR/depth_model if exists (saved during training)\n",
        "    3. None (will download from HuggingFace - requires internet)\n",
        "    \"\"\"\n",
        "    # Check explicit config first\n",
        "    if hasattr(CFG, 'DEPTH_MODEL_PATH') and CFG.DEPTH_MODEL_PATH:\n",
        "        if os.path.exists(CFG.DEPTH_MODEL_PATH):\n",
        "            return CFG.DEPTH_MODEL_PATH\n",
        "        print(f\"Warning: DEPTH_MODEL_PATH set but not found: {CFG.DEPTH_MODEL_PATH}\")\n",
        "    \n",
        "    # Check MODEL_DIR/depth_model\n",
        "    local_path = os.path.join(CFG.MODEL_DIR, \"depth_model\")\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    \n",
        "    # Fallback: will need internet to download from HuggingFace\n",
        "    return None\n",
        "\n",
        "\n",
        "def load_fold_model(\n",
        "    path: str,\n",
        "    grid: Tuple[int, int] = (2, 2),\n",
        "    dropout: float = 0.3,\n",
        "    hidden_ratio: float = 0.25,\n",
        "    use_film: bool = True,\n",
        "    use_attention_pool: bool = True,\n",
        "    train_dead: bool = False,\n",
        "    train_clover: bool = False,\n",
        "    use_vegetation_indices: bool = False,\n",
        "    use_disparity: bool = False,\n",
        "    use_depth: bool = False,\n",
        "    depth_model_size: str = \"small\",\n",
        "    use_depth_attention: bool = False,\n",
        "    use_learnable_aug: bool = False,\n",
        "    learnable_aug_color: bool = True,\n",
        "    learnable_aug_spatial: bool = False,\n",
        "    use_presence_heads: bool = False,\n",
        "    use_ndvi_head: bool = False,\n",
        "    use_height_head: bool = False,\n",
        "    use_species_head: bool = False,\n",
        "    depth_model_path: Optional[str] = None,  # For Kaggle offline\n",
        ") -> nn.Module:\n",
        "    \"\"\"Load a DINOv3Direct model checkpoint.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
        "    \n",
        "    try:\n",
        "        raw_sd = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
        "    except TypeError:\n",
        "        raw_sd = torch.load(path, map_location=\"cpu\")\n",
        "    \n",
        "    sd = _strip_module_prefix(raw_sd)\n",
        "    \n",
        "    # Detect config BEFORE filtering (depth model keys indicate features used)\n",
        "    sd_keys = set(sd.keys())\n",
        "    \n",
        "    # Auto-detect config from checkpoint\n",
        "    detected_config = _detect_model_config(sd_keys)\n",
        "    \n",
        "    # Filter out embedded depth model keys from old checkpoints\n",
        "    # (new code loads depth model separately)\n",
        "    sd = _filter_depth_model_keys(sd)\n",
        "    use_film = detected_config.get(\"use_film\", use_film)\n",
        "    use_attention_pool = detected_config.get(\"use_attention_pool\", use_attention_pool)\n",
        "    train_dead = detected_config.get(\"train_dead\", train_dead)\n",
        "    train_clover = detected_config.get(\"train_clover\", train_clover)\n",
        "    use_vegetation_indices = detected_config.get(\"use_vegetation_indices\", use_vegetation_indices)\n",
        "    use_disparity = detected_config.get(\"use_disparity\", use_disparity)\n",
        "    use_depth = detected_config.get(\"use_depth\", use_depth)\n",
        "    use_depth_attention = detected_config.get(\"use_depth_attention\", use_depth_attention)\n",
        "    use_learnable_aug = detected_config.get(\"use_learnable_aug\", use_learnable_aug)\n",
        "    learnable_aug_color = detected_config.get(\"learnable_aug_color\", learnable_aug_color)\n",
        "    learnable_aug_spatial = detected_config.get(\"learnable_aug_spatial\", learnable_aug_spatial)\n",
        "    use_presence_heads = detected_config.get(\"use_presence_heads\", use_presence_heads)\n",
        "    use_ndvi_head = detected_config.get(\"use_ndvi_head\", use_ndvi_head)\n",
        "    use_height_head = detected_config.get(\"use_height_head\", use_height_head)\n",
        "    use_species_head = detected_config.get(\"use_species_head\", use_species_head)\n",
        "    \n",
        "    # DINOv3Direct is defined locally in this notebook (no src import needed for Kaggle)\n",
        "    \n",
        "    # DINOv3Direct model - use pretrained=False since we load trained weights from checkpoint\n",
        "    # Depth model loaded from local path (Kaggle) or HuggingFace\n",
        "    model = DINOv3Direct(\n",
        "        grid=grid,\n",
        "        pretrained=False,  # Don't download - backbone weights come from checkpoint\n",
        "        dropout=dropout,\n",
        "        hidden_ratio=hidden_ratio,\n",
        "        use_film=use_film,\n",
        "        use_attention_pool=use_attention_pool,\n",
        "        train_dead=train_dead,\n",
        "        train_clover=train_clover,\n",
        "        use_vegetation_indices=use_vegetation_indices,\n",
        "        use_disparity=use_disparity,\n",
        "        use_depth=use_depth,\n",
        "        depth_model_size=depth_model_size,\n",
        "        use_depth_attention=use_depth_attention,\n",
        "        use_learnable_aug=use_learnable_aug,\n",
        "        learnable_aug_color=learnable_aug_color,\n",
        "        learnable_aug_spatial=learnable_aug_spatial,\n",
        "        use_presence_heads=use_presence_heads,\n",
        "        use_ndvi_head=use_ndvi_head,\n",
        "        use_height_head=use_height_head,\n",
        "        use_species_head=use_species_head,\n",
        "        depth_model_path=depth_model_path,  # Local path for Kaggle offline (depth model)\n",
        "    )\n",
        "    \n",
        "    # Load trained weights (depth model already loaded via pretrained=True)\n",
        "    # Use strict=False since depth model keys are filtered out\n",
        "    model.load_state_dict(sd, strict=False)\n",
        "    model = model.to(CFG.DEVICE)\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def find_model_checkpoints(model_dir: str, top_k: Optional[int] = None) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Find DINOv3 model checkpoints.\n",
        "    \n",
        "    Supports:\n",
        "    - Final checkpoints: dinov3_best_fold*.pth, dinov3_top*.pth\n",
        "    - In-progress checkpoints: _topk_fold*_ep*.pth (from --save-top-k)\n",
        "    \n",
        "    Args:\n",
        "        model_dir: Directory containing model checkpoints\n",
        "        top_k: If set, return only top K folds by aR² (from results.json)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (checkpoint_paths, fold_indices)\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Find all checkpoints\n",
        "    all_checkpoints = []\n",
        "    \n",
        "    # Pattern 1: Final checkpoints (dinov3_best_fold*.pth)\n",
        "    for f in os.listdir(model_dir):\n",
        "        if f.startswith(\"dinov3_best_fold\") and f.endswith(\".pth\"):\n",
        "            fold_num = int(f.replace(\"dinov3_best_fold\", \"\").replace(\".pth\", \"\"))\n",
        "            all_checkpoints.append((fold_num, os.path.join(model_dir, f)))\n",
        "    \n",
        "    # Pattern 2: Top-K ranked checkpoints (dinov3_top*_fold*.pth)\n",
        "    if not all_checkpoints:\n",
        "        for f in os.listdir(model_dir):\n",
        "            match = re.match(r\"dinov3_top(\\d+)_fold(\\d+)\\.pth\", f)\n",
        "            if match:\n",
        "                rank, fold_num = int(match.group(1)), int(match.group(2))\n",
        "                all_checkpoints.append((fold_num, os.path.join(model_dir, f)))\n",
        "    \n",
        "    # Pattern 3: In-progress checkpoints (_topk_fold*_ep*.pth) - training not finished\n",
        "    # OR top-k checkpoints from --save-top-k (all from same fold in train-all mode)\n",
        "    if not all_checkpoints:\n",
        "        temp_checkpoints = []\n",
        "        for f in os.listdir(model_dir):\n",
        "            match = re.match(r\"_topk_fold(\\d+)_ep(\\d+)\\.pth\", f)\n",
        "            if match:\n",
        "                fold_num, epoch = int(match.group(1)), int(match.group(2))\n",
        "                temp_checkpoints.append((fold_num, epoch, os.path.join(model_dir, f)))\n",
        "        \n",
        "        if temp_checkpoints:\n",
        "            # Check if train-all mode (all checkpoints are from fold 0)\n",
        "            unique_folds = set(fc[0] for fc in temp_checkpoints)\n",
        "            \n",
        "            if len(unique_folds) == 1:\n",
        "                # Train-all mode: use ALL checkpoints as ensemble (top-k from same fold)\n",
        "                # Sort by epoch descending to prioritize newer checkpoints\n",
        "                temp_checkpoints.sort(key=lambda x: x[1], reverse=True)\n",
        "                for idx, (fold_num, epoch, path) in enumerate(temp_checkpoints):\n",
        "                    all_checkpoints.append((idx, path))  # Use idx as pseudo-fold for sorting\n",
        "                print(f\"Using top-{len(temp_checkpoints)} checkpoints for ensemble (train-all mode)\")\n",
        "            else:\n",
        "                # CV mode: take best epoch per fold\n",
        "                from collections import defaultdict\n",
        "                fold_best = defaultdict(lambda: (-1, None))\n",
        "                for fold_num, epoch, path in temp_checkpoints:\n",
        "                    if epoch > fold_best[fold_num][0]:\n",
        "                        fold_best[fold_num] = (epoch, path)\n",
        "                \n",
        "                for fold_num, (epoch, path) in fold_best.items():\n",
        "                    all_checkpoints.append((fold_num, path))\n",
        "                print(f\"Using in-progress checkpoints (training not finished)\")\n",
        "    \n",
        "    all_checkpoints.sort(key=lambda x: x[0])  # Sort by fold number\n",
        "    \n",
        "    if top_k is None or top_k >= len(all_checkpoints):\n",
        "        # Return all checkpoints\n",
        "        return [c[1] for c in all_checkpoints], [c[0] for c in all_checkpoints]\n",
        "    \n",
        "    # Load results.json to get fold metrics\n",
        "    results_path = os.path.join(model_dir, \"results.json\")\n",
        "    if not os.path.exists(results_path):\n",
        "        print(f\"Warning: results.json not found, using all {len(all_checkpoints)} folds\")\n",
        "        return [c[1] for c in all_checkpoints], [c[0] for c in all_checkpoints]\n",
        "    \n",
        "    with open(results_path) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    fold_results = results.get(\"folds\", [])\n",
        "    if not fold_results:\n",
        "        print(f\"Warning: No fold results in results.json, using all {len(all_checkpoints)} folds\")\n",
        "        return [c[1] for c in all_checkpoints], [c[0] for c in all_checkpoints]\n",
        "    \n",
        "    # Get fold metrics (aR² = best_r2 after our change, or avg_r2 from metrics)\n",
        "    fold_metrics = []\n",
        "    for fr in fold_results:\n",
        "        fold_num = fr[\"fold\"]\n",
        "        # best_r2 is now aR² after our dinov3_train.py change\n",
        "        # For older models, try metrics.avg_r2, then fall back to best_r2\n",
        "        metrics = fr.get(\"metrics\", {})\n",
        "        ar2 = metrics.get(\"avg_r2\", fr.get(\"best_r2\", 0))\n",
        "        fold_metrics.append((fold_num, ar2))\n",
        "    \n",
        "    # Sort by metric (descending) and take top K\n",
        "    fold_metrics.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_folds = set(fm[0] for fm in fold_metrics[:top_k])\n",
        "    \n",
        "    print(f\"Selecting top {top_k} folds by aR²:\")\n",
        "    for fold_num, ar2 in fold_metrics[:top_k]:\n",
        "        print(f\"  Fold {fold_num}: aR²={ar2:.4f}\")\n",
        "    \n",
        "    # Filter checkpoints to top folds\n",
        "    selected = [(fold_num, path) for fold_num, path in all_checkpoints if fold_num in top_folds]\n",
        "    selected.sort(key=lambda x: x[0])\n",
        "    \n",
        "    return [c[1] for c in selected], [c[0] for c in selected]\n",
        "\n",
        "\n",
        "# ==================== INFERENCE ====================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one_view(models: List[nn.Module], loader: DataLoader) -> np.ndarray:\n",
        "    \"\"\"Run inference with fold ensemble.\"\"\"\n",
        "    out_list = []\n",
        "    \n",
        "    for batch in tqdm(loader, desc=\"Predicting\", leave=False):\n",
        "        x_left, x_right, _ = batch\n",
        "        x_left = x_left.to(CFG.DEVICE)\n",
        "        x_right = x_right.to(CFG.DEVICE)\n",
        "        \n",
        "        fold_preds = []\n",
        "        for model in models:\n",
        "            outputs = model(x_left, x_right)\n",
        "            # Handle all output formats:\n",
        "            # - 5 outputs: (green, dead, clover, gdm, total) - basic model\n",
        "            # - 6 outputs: + aux_loss\n",
        "            # - 9 outputs: + aux_loss + state/month/species logits (use_aux_heads)\n",
        "            # - 11 outputs: + aux_loss + presence/ndvi/height/species (use_presence_heads etc.)\n",
        "            # First 5 outputs are always the biomass predictions\n",
        "            if isinstance(outputs, tuple):\n",
        "                green, dead, clover, gdm, total = outputs[:5]\n",
        "            else:\n",
        "                # Single tensor output (shouldn't happen but handle it)\n",
        "                green, dead, clover, gdm, total = outputs[:5]\n",
        "            pred = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
        "            fold_preds.append(pred.float().cpu().numpy())\n",
        "        \n",
        "        avg_pred = np.mean(fold_preds, axis=0)\n",
        "        out_list.append(avg_pred)\n",
        "    \n",
        "    return np.concatenate(out_list, axis=0)\n",
        "\n",
        "\n",
        "def run_inference_tta(models: List[nn.Module], df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Run inference with TTA.\"\"\"\n",
        "    if CFG.USE_TTA:\n",
        "        tta_level = getattr(CFG, 'TTA_LEVEL', 'default')\n",
        "        transforms = get_tta_transforms(level=tta_level)\n",
        "        print(f\"TTA level: {tta_level} ({len(transforms)} views)\")\n",
        "    else:\n",
        "        transforms = [get_val_transform()]\n",
        "        print(\"TTA: disabled (1 view)\")\n",
        "    \n",
        "    all_preds = []\n",
        "    for i, transform in enumerate(transforms):\n",
        "        print(f\"  View {i+1}/{len(transforms)}...\")\n",
        "        ds = TestBiomassDataset(df, CFG.TEST_IMAGE_DIR, transform)\n",
        "        loader = DataLoader(\n",
        "            ds, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS\n",
        "        )\n",
        "        preds = predict_one_view(models, loader)\n",
        "        all_preds.append(preds)\n",
        "    \n",
        "    return np.mean(all_preds, axis=0)\n",
        "\n",
        "\n",
        "def create_submission(final_5: np.ndarray, test_long: pd.DataFrame, test_unique: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Create submission DataFrame with order matching test_long.\"\"\"\n",
        "    green = final_5[:, 0]\n",
        "    dead = final_5[:, 1]\n",
        "    clover = final_5[:, 2]\n",
        "    gdm = final_5[:, 3]\n",
        "    total = final_5[:, 4]\n",
        "\n",
        "    def nnz(x: np.ndarray) -> np.ndarray:\n",
        "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    green, dead, clover, gdm, total = map(nnz, [green, dead, clover, gdm, total])\n",
        "\n",
        "    wide = pd.DataFrame(\n",
        "        {\n",
        "            \"image_path\": test_unique[\"image_path\"],\n",
        "            \"Dry_Green_g\": green,\n",
        "            \"Dry_Dead_g\": dead,\n",
        "            \"Dry_Clover_g\": clover,\n",
        "            \"GDM_g\": gdm,\n",
        "            \"Dry_Total_g\": total,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    long_preds = wide.melt(\n",
        "        id_vars=[\"image_path\"],\n",
        "        value_vars=CFG.ALL_TARGET_COLS,\n",
        "        var_name=\"target_name\",\n",
        "        value_name=\"target\",\n",
        "    )\n",
        "\n",
        "    sub = pd.merge(\n",
        "        test_long[[\"sample_id\", \"image_path\", \"target_name\"]],\n",
        "        long_preds,\n",
        "        on=[\"image_path\", \"target_name\"],\n",
        "        how=\"left\",\n",
        "    )[[\"sample_id\", \"target\"]]\n",
        "\n",
        "    sub[\"target\"] = np.nan_to_num(sub[\"target\"], nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sub.columns = [\"sample_id\", \"value\"]  # Rename for submission format\n",
        "    sub.to_csv(CFG.SUBMISSION_FILE, index=False)\n",
        "    print(f\"Saved: {CFG.SUBMISSION_FILE}\")\n",
        "    print(sub.head(10))\n",
        "    return sub, wide\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run DINOv3 Direct model inference.\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"DINOv3 Direct Model Inference\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Device: {CFG.DEVICE}\")\n",
        "    print(f\"Model dir: {CFG.MODEL_DIR}\")\n",
        "    tta_info = f\"{CFG.TTA_LEVEL} ({len(get_tta_transforms(level=CFG.TTA_LEVEL))} views)\" if CFG.USE_TTA else \"disabled\"\n",
        "    print(f\"TTA: {tta_info}\")\n",
        "    \n",
        "    # Load config from results.json (dinov3_train.py output)\n",
        "    config = load_config_from_results(CFG.MODEL_DIR)\n",
        "    if config:\n",
        "        # dinov3_train.py saves grid as single int\n",
        "        grid_val = config.get(\"grid\", 2)\n",
        "        CFG.GRID = (grid_val, grid_val) if isinstance(grid_val, int) else tuple(grid_val)\n",
        "        # Auto-detect image size from config\n",
        "        CFG.IMG_SIZE = config.get(\"img_size\", CFG.IMG_SIZE)\n",
        "        CFG.DROPOUT = config.get(\"dropout\", CFG.DROPOUT)\n",
        "        CFG.HIDDEN_RATIO = config.get(\"hidden_ratio\", CFG.HIDDEN_RATIO)\n",
        "        CFG.USE_FILM = config.get(\"use_film\", CFG.USE_FILM)\n",
        "        CFG.USE_ATTENTION_POOL = config.get(\"use_attention_pool\", CFG.USE_ATTENTION_POOL)\n",
        "        CFG.TRAIN_DEAD = config.get(\"train_dead\", CFG.TRAIN_DEAD)\n",
        "        CFG.TRAIN_CLOVER = config.get(\"train_clover\", CFG.TRAIN_CLOVER)\n",
        "        CFG.USE_VEGETATION_INDICES = config.get(\"use_vegetation_indices\", CFG.USE_VEGETATION_INDICES)\n",
        "        CFG.USE_DISPARITY = config.get(\"use_disparity\", CFG.USE_DISPARITY)\n",
        "        CFG.USE_DEPTH = config.get(\"use_depth\", CFG.USE_DEPTH)\n",
        "        CFG.DEPTH_MODEL_SIZE = config.get(\"depth_model_size\", CFG.DEPTH_MODEL_SIZE)\n",
        "        CFG.USE_DEPTH_ATTENTION = config.get(\"depth_attention\", CFG.USE_DEPTH_ATTENTION)\n",
        "        CFG.USE_LEARNABLE_AUG = config.get(\"use_learnable_aug\", CFG.USE_LEARNABLE_AUG)\n",
        "        CFG.LEARNABLE_AUG_COLOR = config.get(\"learnable_aug_color\", CFG.LEARNABLE_AUG_COLOR)\n",
        "        CFG.LEARNABLE_AUG_SPATIAL = config.get(\"learnable_aug_spatial\", CFG.LEARNABLE_AUG_SPATIAL)\n",
        "        # New auxiliary heads\n",
        "        CFG.USE_PRESENCE_HEADS = config.get(\"use_presence_heads\", CFG.USE_PRESENCE_HEADS)\n",
        "        CFG.USE_NDVI_HEAD = config.get(\"use_ndvi_head\", CFG.USE_NDVI_HEAD)\n",
        "        CFG.USE_HEIGHT_HEAD = config.get(\"use_height_head\", CFG.USE_HEIGHT_HEAD)\n",
        "        CFG.USE_SPECIES_HEAD = config.get(\"use_species_head\", CFG.USE_SPECIES_HEAD)\n",
        "        print(f\"Loaded config from results.json\")\n",
        "    \n",
        "    print(f\"Model: DINOv3Direct\")\n",
        "    print(f\"Backbone: vit_base_patch16_dinov3\")\n",
        "    print(f\"Grid: {CFG.GRID}\")\n",
        "    print(f\"Image size: {CFG.IMG_SIZE}\")\n",
        "    if CFG.TRAIN_DEAD or CFG.TRAIN_CLOVER:\n",
        "        print(f\"Optional heads: train_dead={CFG.TRAIN_DEAD}, train_clover={CFG.TRAIN_CLOVER}\")\n",
        "    has_extras = (CFG.USE_VEGETATION_INDICES or CFG.USE_DISPARITY or CFG.USE_DEPTH or \n",
        "                  CFG.USE_DEPTH_ATTENTION or CFG.USE_LEARNABLE_AUG or CFG.USE_PRESENCE_HEADS or\n",
        "                  CFG.USE_NDVI_HEAD or CFG.USE_HEIGHT_HEAD)\n",
        "    if has_extras:\n",
        "        extras = []\n",
        "        if CFG.USE_VEGETATION_INDICES:\n",
        "            extras.append(\"Vegetation Indices\")\n",
        "        if CFG.USE_DISPARITY:\n",
        "            extras.append(\"Stereo Disparity\")\n",
        "        if CFG.USE_DEPTH:\n",
        "            extras.append(f\"Depth Stats (DA2-{CFG.DEPTH_MODEL_SIZE})\")\n",
        "        if CFG.USE_DEPTH_ATTENTION:\n",
        "            extras.append(f\"Depth Attention (DA2-{CFG.DEPTH_MODEL_SIZE})\")\n",
        "        if CFG.USE_LEARNABLE_AUG:\n",
        "            aug_types = []\n",
        "            if CFG.LEARNABLE_AUG_COLOR:\n",
        "                aug_types.append(\"color\")\n",
        "            if CFG.LEARNABLE_AUG_SPATIAL:\n",
        "                aug_types.append(\"spatial\")\n",
        "            extras.append(f\"Learnable Aug ({'+'.join(aug_types)})\")\n",
        "        if CFG.USE_PRESENCE_HEADS:\n",
        "            extras.append(\"Presence Heads (Dead/Clover)\")\n",
        "        if CFG.USE_NDVI_HEAD:\n",
        "            extras.append(\"NDVI Head\")\n",
        "        if CFG.USE_HEIGHT_HEAD:\n",
        "            extras.append(\"Height Head\")\n",
        "        print(f\"Innovative features: {', '.join(extras)}\")\n",
        "    \n",
        "    # Find checkpoints (optionally select top K folds)\n",
        "    checkpoints, fold_indices = find_model_checkpoints(CFG.MODEL_DIR, top_k=CFG.TOP_K_FOLDS)\n",
        "    if not checkpoints:\n",
        "        raise ValueError(f\"No checkpoints found in {CFG.MODEL_DIR}\")\n",
        "    if CFG.TOP_K_FOLDS:\n",
        "        print(f\"Using top {len(checkpoints)} folds: {fold_indices}\")\n",
        "    else:\n",
        "        print(f\"Found {len(checkpoints)} fold checkpoints\")\n",
        "    \n",
        "    # Detect depth model path (for Kaggle offline)\n",
        "    depth_model_path = get_depth_model_path()\n",
        "    if depth_model_path:\n",
        "        print(f\"\\nDepth model path: {depth_model_path}\")\n",
        "    else:\n",
        "        print(\"\\nDepth model: Will download from HuggingFace (requires internet)\")\n",
        "    \n",
        "    # Load models\n",
        "    print(\"\\nLoading models...\")\n",
        "    models = []\n",
        "    for ckpt in checkpoints:\n",
        "        model = load_fold_model(\n",
        "            ckpt,\n",
        "            grid=CFG.GRID,\n",
        "            dropout=CFG.DROPOUT,\n",
        "            hidden_ratio=CFG.HIDDEN_RATIO,\n",
        "            use_film=CFG.USE_FILM,\n",
        "            use_attention_pool=CFG.USE_ATTENTION_POOL,\n",
        "            train_dead=CFG.TRAIN_DEAD,\n",
        "            train_clover=CFG.TRAIN_CLOVER,\n",
        "            use_vegetation_indices=CFG.USE_VEGETATION_INDICES,\n",
        "            use_disparity=CFG.USE_DISPARITY,\n",
        "            use_depth=CFG.USE_DEPTH,\n",
        "            depth_model_size=CFG.DEPTH_MODEL_SIZE,\n",
        "            use_depth_attention=CFG.USE_DEPTH_ATTENTION,\n",
        "            use_learnable_aug=CFG.USE_LEARNABLE_AUG,\n",
        "            learnable_aug_color=CFG.LEARNABLE_AUG_COLOR,\n",
        "            learnable_aug_spatial=CFG.LEARNABLE_AUG_SPATIAL,\n",
        "            use_presence_heads=CFG.USE_PRESENCE_HEADS,\n",
        "            use_ndvi_head=CFG.USE_NDVI_HEAD,\n",
        "            use_height_head=CFG.USE_HEIGHT_HEAD,\n",
        "            use_species_head=CFG.USE_SPECIES_HEAD,\n",
        "            depth_model_path=depth_model_path,  # Pass local path for Kaggle\n",
        "        )\n",
        "        models.append(model)\n",
        "        print(f\"  Loaded: {os.path.basename(ckpt)}\")\n",
        "    \n",
        "    # Load test data\n",
        "    print(\"\\nLoading test data...\")\n",
        "    test_long = pd.read_csv(CFG.TEST_CSV)\n",
        "    \n",
        "    # Extract sample_id_prefix for dataset compatibility\n",
        "    if \"sample_id_prefix\" not in test_long.columns:\n",
        "        if \"image_path\" in test_long.columns:\n",
        "            test_long[\"sample_id_prefix\"] = test_long[\"image_path\"].str.extract(r'([A-Z]+\\d+)')[0]\n",
        "        elif \"sample_id\" in test_long.columns:\n",
        "            test_long[\"sample_id_prefix\"] = test_long[\"sample_id\"].str.split(\"__\").str[0]\n",
        "        else:\n",
        "            raise ValueError(\"Cannot find sample_id or image_path column\")\n",
        "    \n",
        "    # Use image_path for dedup to match v1 submission ordering\n",
        "    test_unique = test_long.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
        "    print(f\"Test samples: {len(test_unique)}\")\n",
        "    \n",
        "    # Run inference\n",
        "    print(\"\\nRunning inference...\")\n",
        "    preds = run_inference_tta(models, test_unique)\n",
        "    print(f\"Predictions shape: {preds.shape}\")\n",
        "    \n",
        "    # Constraint check\n",
        "    component_sum = preds[:, 0] + preds[:, 1] + preds[:, 2]\n",
        "    total_pred = preds[:, 4]\n",
        "    diff = np.abs(component_sum - total_pred)\n",
        "    print(f\"\\nConstraint check (G+D+C=T):\")\n",
        "    print(f\"  Max diff: {diff.max():.6f}\")\n",
        "    print(f\"  Mean diff: {diff.mean():.6f}\")\n",
        "    \n",
        "    # Statistics\n",
        "    print(f\"\\nPrediction stats:\")\n",
        "    for i, name in enumerate([\"Green\", \"Dead\", \"Clover\", \"GDM\", \"Total\"]):\n",
        "        print(f\"  {name}: mean={preds[:, i].mean():.2f}, std={preds[:, i].std():.2f}, \"\n",
        "              f\"min={preds[:, i].min():.2f}, max={preds[:, i].max():.2f}\")\n",
        "    \n",
        "    # Create submission\n",
        "    print(\"\\nCreating submission...\")\n",
        "    sub_df, wide_df = create_submission(preds, test_long, test_unique)\n",
        "    print(sub_df.head(10))\n",
        "    \n",
        "    # Cleanup\n",
        "    del models\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Done!\")\n",
        "    print(\"=\"*60)\n",
        "    return sub_df\n",
        "\n",
        "\n",
        "# Run\n",
        "submission = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
