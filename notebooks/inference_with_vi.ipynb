{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSIRO Image2Biomass – Ratio Model Inference\n",
        "\n",
        "This notebook runs inference using Ratio DINOv2/DINOv3 models trained with `train_ratio.py`.\n",
        "\n",
        "**Supports:**\n",
        "- `SoftmaxRatioDINO`: Predicts Total + softmax(Green, Dead, Clover) ratios\n",
        "- `HierarchicalRatioDINO`: Predicts Total → GDM/Total → Green/GDM hierarchically\n",
        "- `DirectDINO`: Predicts Total, Green, GDM directly; derives Dead, Clover\n",
        "- **DINOv2** (518×518) and **DINOv3** (256/512/992) backbones\n",
        "- Multi-fold ensemble\n",
        "- Test-Time Augmentation (TTA)\n",
        "- MPS/CUDA/CPU support\n",
        "\n",
        "**Key advantage**: Components always sum to Total (mathematically guaranteed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kienvu/Desktop/kaggle/biomass/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: mps\n",
            "============================================================\n",
            "Ratio Model Inference\n",
            "============================================================\n",
            "Device: mps\n",
            "Model dir: ../outputs/ratio_20251216_215550\n",
            "TTA: True\n",
            "Loaded config from results.json\n",
            "Model type: direct\n",
            "Backbone: vit_base_patch16_dinov3\n",
            "Grid: (2, 2)\n",
            "Image size: 992\n",
            "Found 5 fold checkpoints\n",
            "\n",
            "Loading models...\n",
            "  Loaded: ratio_best_fold0.pth\n",
            "  Loaded: ratio_best_fold1.pth\n",
            "  Loaded: ratio_best_fold2.pth\n",
            "  Loaded: ratio_best_fold3.pth\n",
            "  Loaded: ratio_best_fold4.pth\n",
            "\n",
            "Loading test data...\n",
            "Test samples: 357\n",
            "\n",
            "Running inference...\n",
            "TTA view 1/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TTA view 2/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TTA view 3/3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions shape: (357, 5)\n",
            "\n",
            "Constraint check (G+D+C=T):\n",
            "  Max diff: 0.000015\n",
            "  Mean diff: 0.000002\n",
            "\n",
            "Prediction stats:\n",
            "  Green: mean=28.24, std=23.72, min=0.00, max=109.63\n",
            "  Dead: mean=11.20, std=6.77, min=1.14, max=31.68\n",
            "  Clover: mean=7.15, std=11.51, min=0.00, max=60.15\n",
            "  GDM: mean=35.39, std=23.28, min=1.84, max=112.52\n",
            "  Total: mean=46.60, std=24.75, min=4.01, max=122.78\n",
            "\n",
            "Creating submission...\n",
            "Saved: submission.csv\n",
            "                    sample_id      value\n",
            "0   ID1011485656__Dry_Green_g  25.797304\n",
            "1    ID1011485656__Dry_Dead_g  24.589590\n",
            "2  ID1011485656__Dry_Clover_g   0.000000\n",
            "3         ID1011485656__GDM_g  25.797304\n",
            "4   ID1011485656__Dry_Total_g  50.386898\n",
            "5   ID1012260530__Dry_Green_g   4.848897\n",
            "6    ID1012260530__Dry_Dead_g   1.139058\n",
            "7  ID1012260530__Dry_Clover_g   1.187443\n",
            "8         ID1012260530__GDM_g   6.036340\n",
            "9   ID1012260530__Dry_Total_g   7.175398\n",
            "\n",
            "============================================================\n",
            "Done!\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    \"\"\"Configuration for Ratio model inference.\"\"\"\n",
        "    \n",
        "    # ==================== LOCAL TESTING MODE ====================\n",
        "    LOCAL_TEST = True\n",
        "    \n",
        "    if LOCAL_TEST:\n",
        "        BASE_PATH = \"../data\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"train.csv\")  # Use train.csv for local OOF testing\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"train\")\n",
        "        # Trained ratio model directory\n",
        "        MODEL_DIR = \"../outputs/ratio_20251216_215550\"\n",
        "    else:\n",
        "        BASE_PATH = \"/kaggle/input/csiro-biomass\"\n",
        "        TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n",
        "        TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"test\")\n",
        "        MODEL_DIR = \"/kaggle/input/your-ratio-model\"  # Update for Kaggle\n",
        "    \n",
        "    # Backbone config (auto-loaded from results.json)\n",
        "    BACKBONE = \"vit_base_patch14_reg4_dinov2.lvd142m\"\n",
        "    \n",
        "    # ==================== INFERENCE SETTINGS ====================\n",
        "    SUBMISSION_FILE = \"submission.csv\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Updated below\n",
        "    BATCH_SIZE = 1\n",
        "    NUM_WORKERS = 0\n",
        "    \n",
        "    # Model architecture params (auto-loaded from results.json)\n",
        "    MODEL_TYPE = \"hierarchical\"  # \"softmax\", \"hierarchical\", or \"direct\"\n",
        "    DROPOUT = 0.2\n",
        "    HIDDEN_RATIO = 0.5\n",
        "    GRID = (2, 2)\n",
        "    USE_FILM = True\n",
        "    USE_ATTENTION_POOL = True\n",
        "    RATIO_TEMPERATURE = 1.0\n",
        "    IMG_SIZE = 518  # Auto-detected: 518 for DINOv2, 256/512/992 for DINOv3\n",
        "    \n",
        "    # TTA settings\n",
        "    USE_TTA = True\n",
        "    \n",
        "    ALL_TARGET_COLS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
        "\n",
        "\n",
        "# Backbone presets for common configurations\n",
        "BACKBONE_PRESETS = {\n",
        "    \"vit_base_patch14_reg4_dinov2.lvd142m\": {\"img_size\": 518, \"grid\": (2, 2)},\n",
        "    \"vit_base_patch16_dinov3\": {\"img_size\": 512, \"grid\": (2, 2)},\n",
        "    \"dinov3_base\": {\"img_size\": 512, \"grid\": (2, 2)},\n",
        "}\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Get best available device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Update device\n",
        "CFG.DEVICE = get_device()\n",
        "\n",
        "print(f\"Device: {CFG.DEVICE}\")\n",
        "\n",
        "\n",
        "def load_config_from_results(model_dir: str) -> Dict:\n",
        "    \"\"\"Load training config from results.json if available.\"\"\"\n",
        "    results_path = os.path.join(model_dir, \"results.json\")\n",
        "    if os.path.exists(results_path):\n",
        "        with open(results_path) as f:\n",
        "            results = json.load(f)\n",
        "        return results.get(\"config\", {})\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ==================== DATASET ====================\n",
        "\n",
        "class TestBiomassDataset(Dataset):\n",
        "    \"\"\"Dataset for test/inference.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, image_dir: str, transform: A.Compose):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:\n",
        "        row = self.df.iloc[idx]\n",
        "        sample_id = row[\"sample_id_prefix\"]\n",
        "        \n",
        "        img_path = os.path.join(self.image_dir, f\"{sample_id}.jpg\")\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        H, W = img.shape[:2]\n",
        "        mid = W // 2\n",
        "        img_left = img[:, :mid, :]\n",
        "        img_right = img[:, mid:, :]\n",
        "        \n",
        "        if self.transform:\n",
        "            aug_left = self.transform(image=img_left)\n",
        "            aug_right = self.transform(image=img_right)\n",
        "            img_left = aug_left[\"image\"]\n",
        "            img_right = aug_right[\"image\"]\n",
        "        \n",
        "        return img_left, img_right, sample_id\n",
        "\n",
        "\n",
        "def get_val_transform(img_size: Optional[int] = None) -> A.Compose:\n",
        "    if img_size is None:\n",
        "        img_size = CFG.IMG_SIZE\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_tta_transforms(img_size: Optional[int] = None) -> List[A.Compose]:\n",
        "    \"\"\"TTA transforms: original + hflip + brightness.\"\"\"\n",
        "    if img_size is None:\n",
        "        img_size = CFG.IMG_SIZE\n",
        "    base = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    hflip = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.HorizontalFlip(p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    bright = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    return [base, hflip, bright]\n",
        "\n",
        "\n",
        "# ==================== MODEL COMPONENTS ====================\n",
        "\n",
        "def _build_dino_by_name(backbone_name: str, pretrained: bool = False):\n",
        "    model = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
        "    feat_dim = model.embed_dim\n",
        "    if hasattr(model, \"patch_embed\"):\n",
        "        input_res = model.patch_embed.img_size\n",
        "        if isinstance(input_res, (tuple, list)):\n",
        "            input_res = input_res[0]\n",
        "    else:\n",
        "        input_res = 518\n",
        "    return model, feat_dim, input_res\n",
        "\n",
        "\n",
        "def _make_edges(length: int, n_parts: int) -> List[Tuple[int, int]]:\n",
        "    step = length // n_parts\n",
        "    edges = [(i * step, (i + 1) * step) for i in range(n_parts)]\n",
        "    edges[-1] = (edges[-1][0], length)\n",
        "    return edges\n",
        "\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, in_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        hidden = max(64, in_dim // 2)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, in_dim * 2),\n",
        "        )\n",
        "    \n",
        "    def forward(self, context: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        gb = self.mlp(context)\n",
        "        gamma, beta = torch.chunk(gb, 2, dim=1)\n",
        "        return gamma, beta\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.scale = dim ** -0.5\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        q = self.query(x.mean(dim=1, keepdim=True))\n",
        "        k = self.key(x)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        return (attn @ x).squeeze(1)\n",
        "\n",
        "\n",
        "# ==================== RATIO MODELS ====================\n",
        "\n",
        "class SoftmaxRatioDINO(nn.Module):\n",
        "    \"\"\"Softmax Ratio Model: Predict Total + component ratios.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone_name: str = \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
        "        grid: Tuple[int, int] = (2, 2),\n",
        "        pretrained: bool = False,\n",
        "        dropout: float = 0.2,\n",
        "        hidden_ratio: float = 0.5,\n",
        "        use_film: bool = True,\n",
        "        use_attention_pool: bool = True,\n",
        "        ratio_temperature: float = 1.0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone, feat_dim, input_res = _build_dino_by_name(backbone_name, pretrained)\n",
        "        self.input_res = int(input_res)\n",
        "        self.feat_dim = feat_dim\n",
        "        self.grid = tuple(grid)\n",
        "        self.use_film = use_film\n",
        "        self.use_attention_pool = use_attention_pool\n",
        "        self.ratio_temperature = ratio_temperature\n",
        "        \n",
        "        if use_film:\n",
        "            self.film_left = FiLM(feat_dim)\n",
        "            self.film_right = FiLM(feat_dim)\n",
        "        \n",
        "        if use_attention_pool:\n",
        "            self.attn_pool_left = AttentionPooling(feat_dim)\n",
        "            self.attn_pool_right = AttentionPooling(feat_dim)\n",
        "        \n",
        "        self.combined_dim = feat_dim * 2\n",
        "        hidden_dim = max(64, int(self.combined_dim * hidden_ratio))\n",
        "        \n",
        "        self.shared_proj = nn.Sequential(\n",
        "            nn.LayerNorm(self.combined_dim),\n",
        "            nn.Linear(self.combined_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "        def _make_head(in_dim: int, out_dim: int = 1) -> nn.Sequential:\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(in_dim, in_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout * 0.5),\n",
        "                nn.Linear(in_dim, out_dim),\n",
        "            )\n",
        "        \n",
        "        self.head_total = _make_head(hidden_dim, 1)\n",
        "        self.head_ratios = _make_head(hidden_dim, 3)\n",
        "        self.softplus = nn.Softplus(beta=1.0)\n",
        "    \n",
        "    def _collect_tiles(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        _, C, H, W = x.shape\n",
        "        r, c = self.grid\n",
        "        rows = _make_edges(H, r)\n",
        "        cols = _make_edges(W, c)\n",
        "        tiles = []\n",
        "        for (y0, y1) in rows:\n",
        "            for (x0, x1) in cols:\n",
        "                tile = x[:, :, y0:y1, x0:x1]\n",
        "                tile = F.interpolate(tile, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
        "                tiles.append(tile)\n",
        "        return tiles\n",
        "    \n",
        "    def _extract_tiles_fused(self, x_left: torch.Tensor, x_right: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B = x_left.size(0)\n",
        "        tiles_left = self._collect_tiles(x_left)\n",
        "        tiles_right = self._collect_tiles(x_right)\n",
        "        num_tiles = len(tiles_left)\n",
        "        \n",
        "        all_tiles = torch.cat(tiles_left + tiles_right, dim=0)\n",
        "        all_feats = self.backbone(all_tiles)\n",
        "        \n",
        "        total_tiles = 2 * num_tiles\n",
        "        all_feats = all_feats.view(total_tiles, B, -1).permute(1, 0, 2)\n",
        "        feats_left = all_feats[:, :num_tiles, :]\n",
        "        feats_right = all_feats[:, num_tiles:, :]\n",
        "        return feats_left, feats_right\n",
        "    \n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
        "        tiles_left, tiles_right = self._extract_tiles_fused(x_left, x_right)\n",
        "        \n",
        "        ctx_left = tiles_left.mean(dim=1)\n",
        "        ctx_right = tiles_right.mean(dim=1)\n",
        "        \n",
        "        if self.use_film:\n",
        "            gamma_l, beta_l = self.film_left(ctx_right)\n",
        "            gamma_r, beta_r = self.film_right(ctx_left)\n",
        "            tiles_left = tiles_left * (1 + gamma_l.unsqueeze(1)) + beta_l.unsqueeze(1)\n",
        "            tiles_right = tiles_right * (1 + gamma_r.unsqueeze(1)) + beta_r.unsqueeze(1)\n",
        "        \n",
        "        if self.use_attention_pool:\n",
        "            f_l = self.attn_pool_left(tiles_left)\n",
        "            f_r = self.attn_pool_right(tiles_right)\n",
        "        else:\n",
        "            f_l = tiles_left.mean(dim=1)\n",
        "            f_r = tiles_right.mean(dim=1)\n",
        "        \n",
        "        f = torch.cat([f_l, f_r], dim=1)\n",
        "        f = self.shared_proj(f)\n",
        "        \n",
        "        total = self.softplus(self.head_total(f))\n",
        "        logits = self.head_ratios(f)\n",
        "        ratios = F.softmax(logits / self.ratio_temperature, dim=1)\n",
        "        green_ratio, dead_ratio, clover_ratio = ratios[:, 0:1], ratios[:, 1:2], ratios[:, 2:3]\n",
        "        \n",
        "        green = total * green_ratio\n",
        "        dead = total * dead_ratio\n",
        "        clover = total * clover_ratio\n",
        "        gdm = green + clover\n",
        "        \n",
        "        return green, dead, clover, gdm, total\n",
        "\n",
        "\n",
        "class HierarchicalRatioDINO(nn.Module):\n",
        "    \"\"\"Hierarchical Ratio Model: Total → GDM/Total → Green/GDM.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone_name: str = \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
        "        grid: Tuple[int, int] = (2, 2),\n",
        "        pretrained: bool = False,\n",
        "        dropout: float = 0.2,\n",
        "        hidden_ratio: float = 0.5,\n",
        "        use_film: bool = True,\n",
        "        use_attention_pool: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone, feat_dim, input_res = _build_dino_by_name(backbone_name, pretrained)\n",
        "        self.input_res = int(input_res)\n",
        "        self.feat_dim = feat_dim\n",
        "        self.grid = tuple(grid)\n",
        "        self.use_film = use_film\n",
        "        self.use_attention_pool = use_attention_pool\n",
        "        \n",
        "        if use_film:\n",
        "            self.film_left = FiLM(feat_dim)\n",
        "            self.film_right = FiLM(feat_dim)\n",
        "        \n",
        "        if use_attention_pool:\n",
        "            self.attn_pool_left = AttentionPooling(feat_dim)\n",
        "            self.attn_pool_right = AttentionPooling(feat_dim)\n",
        "        \n",
        "        self.combined_dim = feat_dim * 2\n",
        "        hidden_dim = max(64, int(self.combined_dim * hidden_ratio))\n",
        "        \n",
        "        self.shared_proj = nn.Sequential(\n",
        "            nn.LayerNorm(self.combined_dim),\n",
        "            nn.Linear(self.combined_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "        def _make_head(in_dim: int, out_dim: int = 1) -> nn.Sequential:\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(in_dim, in_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout * 0.5),\n",
        "                nn.Linear(in_dim, out_dim),\n",
        "            )\n",
        "        \n",
        "        self.head_total = _make_head(hidden_dim, 1)\n",
        "        self.head_alive_ratio = _make_head(hidden_dim, 1)\n",
        "        self.head_green_ratio = _make_head(hidden_dim, 1)\n",
        "        self.softplus = nn.Softplus(beta=1.0)\n",
        "    \n",
        "    def _collect_tiles(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        _, C, H, W = x.shape\n",
        "        r, c = self.grid\n",
        "        rows = _make_edges(H, r)\n",
        "        cols = _make_edges(W, c)\n",
        "        tiles = []\n",
        "        for (y0, y1) in rows:\n",
        "            for (x0, x1) in cols:\n",
        "                tile = x[:, :, y0:y1, x0:x1]\n",
        "                tile = F.interpolate(tile, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
        "                tiles.append(tile)\n",
        "        return tiles\n",
        "    \n",
        "    def _extract_tiles_fused(self, x_left: torch.Tensor, x_right: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B = x_left.size(0)\n",
        "        tiles_left = self._collect_tiles(x_left)\n",
        "        tiles_right = self._collect_tiles(x_right)\n",
        "        num_tiles = len(tiles_left)\n",
        "        \n",
        "        all_tiles = torch.cat(tiles_left + tiles_right, dim=0)\n",
        "        all_feats = self.backbone(all_tiles)\n",
        "        \n",
        "        total_tiles = 2 * num_tiles\n",
        "        all_feats = all_feats.view(total_tiles, B, -1).permute(1, 0, 2)\n",
        "        feats_left = all_feats[:, :num_tiles, :]\n",
        "        feats_right = all_feats[:, num_tiles:, :]\n",
        "        return feats_left, feats_right\n",
        "    \n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
        "        tiles_left, tiles_right = self._extract_tiles_fused(x_left, x_right)\n",
        "        \n",
        "        ctx_left = tiles_left.mean(dim=1)\n",
        "        ctx_right = tiles_right.mean(dim=1)\n",
        "        \n",
        "        if self.use_film:\n",
        "            gamma_l, beta_l = self.film_left(ctx_right)\n",
        "            gamma_r, beta_r = self.film_right(ctx_left)\n",
        "            tiles_left = tiles_left * (1 + gamma_l.unsqueeze(1)) + beta_l.unsqueeze(1)\n",
        "            tiles_right = tiles_right * (1 + gamma_r.unsqueeze(1)) + beta_r.unsqueeze(1)\n",
        "        \n",
        "        if self.use_attention_pool:\n",
        "            f_l = self.attn_pool_left(tiles_left)\n",
        "            f_r = self.attn_pool_right(tiles_right)\n",
        "        else:\n",
        "            f_l = tiles_left.mean(dim=1)\n",
        "            f_r = tiles_right.mean(dim=1)\n",
        "        \n",
        "        f = torch.cat([f_l, f_r], dim=1)\n",
        "        f = self.shared_proj(f)\n",
        "        \n",
        "        total = self.softplus(self.head_total(f))\n",
        "        alive_ratio = torch.sigmoid(self.head_alive_ratio(f))\n",
        "        green_ratio = torch.sigmoid(self.head_green_ratio(f))\n",
        "        \n",
        "        gdm = total * alive_ratio\n",
        "        dead = total - gdm\n",
        "        green = gdm * green_ratio\n",
        "        clover = gdm - green\n",
        "        \n",
        "        dead = F.relu(dead)\n",
        "        clover = F.relu(clover)\n",
        "        \n",
        "        return green, dead, clover, gdm, total\n",
        "\n",
        "\n",
        "class DirectDINO(nn.Module):\n",
        "    \"\"\"Direct Model: Predict Total, Green, GDM directly; derive Dead, Clover.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone_name: str = \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
        "        grid: Tuple[int, int] = (2, 2),\n",
        "        pretrained: bool = False,\n",
        "        dropout: float = 0.2,\n",
        "        hidden_ratio: float = 0.5,\n",
        "        use_film: bool = True,\n",
        "        use_attention_pool: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone, feat_dim, input_res = _build_dino_by_name(backbone_name, pretrained)\n",
        "        self.input_res = int(input_res)\n",
        "        self.feat_dim = feat_dim\n",
        "        self.grid = tuple(grid)\n",
        "        self.use_film = use_film\n",
        "        self.use_attention_pool = use_attention_pool\n",
        "        \n",
        "        if use_film:\n",
        "            self.film_left = FiLM(feat_dim)\n",
        "            self.film_right = FiLM(feat_dim)\n",
        "        \n",
        "        if use_attention_pool:\n",
        "            self.attn_pool_left = AttentionPooling(feat_dim)\n",
        "            self.attn_pool_right = AttentionPooling(feat_dim)\n",
        "        \n",
        "        self.combined_dim = feat_dim * 2\n",
        "        hidden_dim = max(64, int(self.combined_dim * hidden_ratio))\n",
        "        \n",
        "        self.shared_proj = nn.Sequential(\n",
        "            nn.LayerNorm(self.combined_dim),\n",
        "            nn.Linear(self.combined_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "        def _make_head(in_dim: int, out_dim: int = 1) -> nn.Sequential:\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(in_dim, in_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout * 0.5),\n",
        "                nn.Linear(in_dim, out_dim),\n",
        "            )\n",
        "        \n",
        "        self.head_total = _make_head(hidden_dim, 1)\n",
        "        self.head_green = _make_head(hidden_dim, 1)\n",
        "        self.head_gdm = _make_head(hidden_dim, 1)\n",
        "        self.softplus = nn.Softplus(beta=1.0)\n",
        "    \n",
        "    def _collect_tiles(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        _, C, H, W = x.shape\n",
        "        r, c = self.grid\n",
        "        rows = _make_edges(H, r)\n",
        "        cols = _make_edges(W, c)\n",
        "        tiles = []\n",
        "        for (y0, y1) in rows:\n",
        "            for (x0, x1) in cols:\n",
        "                tile = x[:, :, y0:y1, x0:x1]\n",
        "                tile = F.interpolate(tile, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
        "                tiles.append(tile)\n",
        "        return tiles\n",
        "    \n",
        "    def _extract_tiles_fused(self, x_left: torch.Tensor, x_right: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B = x_left.size(0)\n",
        "        tiles_left = self._collect_tiles(x_left)\n",
        "        tiles_right = self._collect_tiles(x_right)\n",
        "        num_tiles = len(tiles_left)\n",
        "        \n",
        "        all_tiles = torch.cat(tiles_left + tiles_right, dim=0)\n",
        "        all_feats = self.backbone(all_tiles)\n",
        "        \n",
        "        total_tiles = 2 * num_tiles\n",
        "        all_feats = all_feats.view(total_tiles, B, -1).permute(1, 0, 2)\n",
        "        feats_left = all_feats[:, :num_tiles, :]\n",
        "        feats_right = all_feats[:, num_tiles:, :]\n",
        "        return feats_left, feats_right\n",
        "    \n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
        "        tiles_left, tiles_right = self._extract_tiles_fused(x_left, x_right)\n",
        "        \n",
        "        ctx_left = tiles_left.mean(dim=1)\n",
        "        ctx_right = tiles_right.mean(dim=1)\n",
        "        \n",
        "        if self.use_film:\n",
        "            gamma_l, beta_l = self.film_left(ctx_right)\n",
        "            gamma_r, beta_r = self.film_right(ctx_left)\n",
        "            tiles_left = tiles_left * (1 + gamma_l.unsqueeze(1)) + beta_l.unsqueeze(1)\n",
        "            tiles_right = tiles_right * (1 + gamma_r.unsqueeze(1)) + beta_r.unsqueeze(1)\n",
        "        \n",
        "        if self.use_attention_pool:\n",
        "            f_l = self.attn_pool_left(tiles_left)\n",
        "            f_r = self.attn_pool_right(tiles_right)\n",
        "        else:\n",
        "            f_l = tiles_left.mean(dim=1)\n",
        "            f_r = tiles_right.mean(dim=1)\n",
        "        \n",
        "        f = torch.cat([f_l, f_r], dim=1)\n",
        "        f = self.shared_proj(f)\n",
        "        \n",
        "        # Predict Total, Green, GDM directly (all positive via softplus)\n",
        "        total_raw = self.softplus(self.head_total(f))\n",
        "        green_raw = self.softplus(self.head_green(f))\n",
        "        gdm_raw = self.softplus(self.head_gdm(f))\n",
        "        \n",
        "        # Enforce constraints: Total >= GDM >= Green >= 0\n",
        "        total = total_raw\n",
        "        gdm = torch.minimum(gdm_raw, total)\n",
        "        green = torch.minimum(green_raw, gdm)\n",
        "        \n",
        "        # Derive Dead and Clover\n",
        "        dead = total - gdm\n",
        "        clover = gdm - green\n",
        "        \n",
        "        # Ensure non-negative (numerical safety)\n",
        "        dead = F.relu(dead)\n",
        "        clover = F.relu(clover)\n",
        "        \n",
        "        return green, dead, clover, gdm, total\n",
        "\n",
        "\n",
        "# ==================== MODEL LOADING ====================\n",
        "\n",
        "def _strip_module_prefix(sd: dict) -> dict:\n",
        "    if not sd:\n",
        "        return sd\n",
        "    keys = list(sd.keys())\n",
        "    if all(k.startswith(\"module.\") for k in keys):\n",
        "        return {k[len(\"module.\"):]: v for k, v in sd.items()}\n",
        "    return sd\n",
        "\n",
        "\n",
        "def _detect_model_type(sd_keys: set) -> str:\n",
        "    if any(k.startswith(\"head_alive_ratio.\") for k in sd_keys):\n",
        "        return \"hierarchical\"\n",
        "    elif any(k.startswith(\"head_ratios.\") for k in sd_keys):\n",
        "        return \"softmax\"\n",
        "    elif any(k.startswith(\"head_green.\") for k in sd_keys) and any(k.startswith(\"head_gdm.\") for k in sd_keys):\n",
        "        return \"direct\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type\")\n",
        "\n",
        "\n",
        "def _detect_model_config(sd_keys: set) -> dict:\n",
        "    return {\n",
        "        \"use_film\": any(k.startswith(\"film_left.\") for k in sd_keys),\n",
        "        \"use_attention_pool\": any(k.startswith(\"attn_pool_left.\") for k in sd_keys),\n",
        "    }\n",
        "\n",
        "\n",
        "def load_fold_model(\n",
        "    path: str,\n",
        "    backbone_name: str,\n",
        "    model_type: str = \"hierarchical\",\n",
        "    grid: Tuple[int, int] = (2, 2),\n",
        "    dropout: float = 0.2,\n",
        "    hidden_ratio: float = 0.5,\n",
        "    use_film: bool = True,\n",
        "    use_attention_pool: bool = True,\n",
        "    ratio_temperature: float = 1.0,\n",
        ") -> nn.Module:\n",
        "    \"\"\"Load a ratio model checkpoint.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
        "    \n",
        "    try:\n",
        "        raw_sd = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
        "    except TypeError:\n",
        "        raw_sd = torch.load(path, map_location=\"cpu\")\n",
        "    \n",
        "    sd = _strip_module_prefix(raw_sd)\n",
        "    sd_keys = set(sd.keys())\n",
        "    \n",
        "    detected_type = _detect_model_type(sd_keys)\n",
        "    if detected_type != model_type:\n",
        "        print(f\"  Auto-detected model type: {detected_type}\")\n",
        "        model_type = detected_type\n",
        "    \n",
        "    detected_config = _detect_model_config(sd_keys)\n",
        "    use_film = detected_config.get(\"use_film\", use_film)\n",
        "    use_attention_pool = detected_config.get(\"use_attention_pool\", use_attention_pool)\n",
        "    \n",
        "    if model_type == \"softmax\":\n",
        "        model = SoftmaxRatioDINO(\n",
        "            backbone_name=backbone_name, grid=grid, pretrained=False,\n",
        "            dropout=dropout, hidden_ratio=hidden_ratio,\n",
        "            use_film=use_film, use_attention_pool=use_attention_pool,\n",
        "            ratio_temperature=ratio_temperature,\n",
        "        )\n",
        "    elif model_type == \"direct\":\n",
        "        model = DirectDINO(\n",
        "            backbone_name=backbone_name, grid=grid, pretrained=False,\n",
        "            dropout=dropout, hidden_ratio=hidden_ratio,\n",
        "            use_film=use_film, use_attention_pool=use_attention_pool,\n",
        "        )\n",
        "    else:\n",
        "        model = HierarchicalRatioDINO(\n",
        "            backbone_name=backbone_name, grid=grid, pretrained=False,\n",
        "            dropout=dropout, hidden_ratio=hidden_ratio,\n",
        "            use_film=use_film, use_attention_pool=use_attention_pool,\n",
        "        )\n",
        "    \n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    model = model.to(CFG.DEVICE)\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def find_model_checkpoints(model_dir: str) -> List[str]:\n",
        "    checkpoints = []\n",
        "    for f in os.listdir(model_dir):\n",
        "        if f.startswith(\"ratio_best_fold\") and f.endswith(\".pth\"):\n",
        "            checkpoints.append(os.path.join(model_dir, f))\n",
        "    checkpoints.sort()\n",
        "    return checkpoints\n",
        "\n",
        "\n",
        "# ==================== INFERENCE ====================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one_view(models: List[nn.Module], loader: DataLoader) -> np.ndarray:\n",
        "    \"\"\"Run inference with fold ensemble.\"\"\"\n",
        "    out_list = []\n",
        "    \n",
        "    for batch in tqdm(loader, desc=\"Predicting\", leave=False):\n",
        "        x_left, x_right, _ = batch\n",
        "        x_left = x_left.to(CFG.DEVICE)\n",
        "        x_right = x_right.to(CFG.DEVICE)\n",
        "        \n",
        "        fold_preds = []\n",
        "        for model in models:\n",
        "            green, dead, clover, gdm, total = model(x_left, x_right)\n",
        "            pred = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
        "            fold_preds.append(pred.float().cpu().numpy())\n",
        "        \n",
        "        avg_pred = np.mean(fold_preds, axis=0)\n",
        "        out_list.append(avg_pred)\n",
        "    \n",
        "    return np.concatenate(out_list, axis=0)\n",
        "\n",
        "\n",
        "def run_inference_tta(models: List[nn.Module], df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Run inference with TTA.\"\"\"\n",
        "    transforms = get_tta_transforms() if CFG.USE_TTA else [get_val_transform()]\n",
        "    \n",
        "    all_preds = []\n",
        "    for i, transform in enumerate(transforms):\n",
        "        print(f\"TTA view {i+1}/{len(transforms)}...\")\n",
        "        ds = TestBiomassDataset(df, CFG.TEST_IMAGE_DIR, transform)\n",
        "        loader = DataLoader(\n",
        "            ds, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS\n",
        "        )\n",
        "        preds = predict_one_view(models, loader)\n",
        "        all_preds.append(preds)\n",
        "    \n",
        "    return np.mean(all_preds, axis=0)\n",
        "\n",
        "\n",
        "def create_submission(preds: np.ndarray, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create submission DataFrame.\"\"\"\n",
        "    green = np.maximum(0, preds[:, 0])\n",
        "    dead = np.maximum(0, preds[:, 1])\n",
        "    clover = np.maximum(0, preds[:, 2])\n",
        "    gdm = np.maximum(0, preds[:, 3])\n",
        "    total = np.maximum(0, preds[:, 4])\n",
        "    \n",
        "    rows = []\n",
        "    for i, sample_id in enumerate(df[\"sample_id_prefix\"]):\n",
        "        rows.append({\"sample_id\": f\"{sample_id}__Dry_Green_g\", \"value\": green[i]})\n",
        "        rows.append({\"sample_id\": f\"{sample_id}__Dry_Dead_g\", \"value\": dead[i]})\n",
        "        rows.append({\"sample_id\": f\"{sample_id}__Dry_Clover_g\", \"value\": clover[i]})\n",
        "        rows.append({\"sample_id\": f\"{sample_id}__GDM_g\", \"value\": gdm[i]})\n",
        "        rows.append({\"sample_id\": f\"{sample_id}__Dry_Total_g\", \"value\": total[i]})\n",
        "    \n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run ratio model inference.\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Ratio Model Inference\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Device: {CFG.DEVICE}\")\n",
        "    print(f\"Model dir: {CFG.MODEL_DIR}\")\n",
        "    print(f\"TTA: {CFG.USE_TTA}\")\n",
        "    \n",
        "    # Load config\n",
        "    config = load_config_from_results(CFG.MODEL_DIR)\n",
        "    if config:\n",
        "        CFG.MODEL_TYPE = config.get(\"model_type\", CFG.MODEL_TYPE)\n",
        "        CFG.BACKBONE = config.get(\"backbone\", CFG.BACKBONE)\n",
        "        CFG.GRID = tuple(config.get(\"grid\", list(CFG.GRID)))\n",
        "        CFG.DROPOUT = config.get(\"dropout\", CFG.DROPOUT)\n",
        "        CFG.HIDDEN_RATIO = config.get(\"hidden_ratio\", CFG.HIDDEN_RATIO)\n",
        "        CFG.USE_FILM = config.get(\"use_film\", CFG.USE_FILM)\n",
        "        CFG.USE_ATTENTION_POOL = config.get(\"use_attention_pool\", CFG.USE_ATTENTION_POOL)\n",
        "        CFG.IMG_SIZE = config.get(\"img_size\", CFG.IMG_SIZE)\n",
        "        print(f\"Loaded config from results.json\")\n",
        "    \n",
        "    # Check if img_size was in config\n",
        "    if \"img_size\" not in config:\n",
        "        print(f\"⚠️  WARNING: img_size not in results.json - using default/preset\")\n",
        "        # Apply backbone preset\n",
        "        if CFG.BACKBONE in BACKBONE_PRESETS:\n",
        "            preset = BACKBONE_PRESETS[CFG.BACKBONE]\n",
        "            CFG.IMG_SIZE = preset[\"img_size\"]\n",
        "            print(f\"  Applied preset for {CFG.BACKBONE}: img_size={CFG.IMG_SIZE}\")\n",
        "        # Auto-detect DINOv3 backbones\n",
        "        if \"dinov3\" in CFG.BACKBONE.lower() and CFG.IMG_SIZE == 518:\n",
        "            CFG.IMG_SIZE = 512  # Default for DINOv3\n",
        "            print(f\"  Auto-adjusted IMG_SIZE to {CFG.IMG_SIZE} for DINOv3\")\n",
        "        print(f\"  ⚠️  If training used different img_size (e.g., 992), set CFG.IMG_SIZE manually!\")\n",
        "    \n",
        "    print(f\"Model type: {CFG.MODEL_TYPE}\")\n",
        "    print(f\"Backbone: {CFG.BACKBONE}\")\n",
        "    print(f\"Grid: {CFG.GRID}\")\n",
        "    print(f\"Image size: {CFG.IMG_SIZE}\")\n",
        "    \n",
        "    # Find checkpoints\n",
        "    checkpoints = find_model_checkpoints(CFG.MODEL_DIR)\n",
        "    if not checkpoints:\n",
        "        raise ValueError(f\"No checkpoints found in {CFG.MODEL_DIR}\")\n",
        "    print(f\"Found {len(checkpoints)} fold checkpoints\")\n",
        "    \n",
        "    # Load models\n",
        "    print(\"\\nLoading models...\")\n",
        "    models = []\n",
        "    for ckpt in checkpoints:\n",
        "        model = load_fold_model(\n",
        "            ckpt,\n",
        "            backbone_name=CFG.BACKBONE,\n",
        "            model_type=CFG.MODEL_TYPE,\n",
        "            grid=CFG.GRID,\n",
        "            dropout=CFG.DROPOUT,\n",
        "            hidden_ratio=CFG.HIDDEN_RATIO,\n",
        "            use_film=CFG.USE_FILM,\n",
        "            use_attention_pool=CFG.USE_ATTENTION_POOL,\n",
        "        )\n",
        "        models.append(model)\n",
        "        print(f\"  Loaded: {os.path.basename(ckpt)}\")\n",
        "    \n",
        "    # Load test data\n",
        "    print(\"\\nLoading test data...\")\n",
        "    test_long = pd.read_csv(CFG.TEST_CSV)\n",
        "    \n",
        "    # Extract sample_id_prefix\n",
        "    if \"sample_id_prefix\" not in test_long.columns:\n",
        "        if \"image_path\" in test_long.columns:\n",
        "            test_long[\"sample_id_prefix\"] = test_long[\"image_path\"].str.extract(r'([A-Z]+\\d+)')[0]\n",
        "        elif \"sample_id\" in test_long.columns:\n",
        "            test_long[\"sample_id_prefix\"] = test_long[\"sample_id\"].str.split(\"__\").str[0]\n",
        "        else:\n",
        "            raise ValueError(\"Cannot find sample_id or image_path column\")\n",
        "    \n",
        "    test_unique = test_long.drop_duplicates(subset=[\"sample_id_prefix\"]).reset_index(drop=True)\n",
        "    print(f\"Test samples: {len(test_unique)}\")\n",
        "    \n",
        "    # Run inference\n",
        "    print(\"\\nRunning inference...\")\n",
        "    preds = run_inference_tta(models, test_unique)\n",
        "    print(f\"Predictions shape: {preds.shape}\")\n",
        "    \n",
        "    # Constraint check\n",
        "    component_sum = preds[:, 0] + preds[:, 1] + preds[:, 2]\n",
        "    total_pred = preds[:, 4]\n",
        "    diff = np.abs(component_sum - total_pred)\n",
        "    print(f\"\\nConstraint check (G+D+C=T):\")\n",
        "    print(f\"  Max diff: {diff.max():.6f}\")\n",
        "    print(f\"  Mean diff: {diff.mean():.6f}\")\n",
        "    \n",
        "    # Statistics\n",
        "    print(f\"\\nPrediction stats:\")\n",
        "    for i, name in enumerate([\"Green\", \"Dead\", \"Clover\", \"GDM\", \"Total\"]):\n",
        "        print(f\"  {name}: mean={preds[:, i].mean():.2f}, std={preds[:, i].std():.2f}, \"\n",
        "              f\"min={preds[:, i].min():.2f}, max={preds[:, i].max():.2f}\")\n",
        "    \n",
        "    # Create submission\n",
        "    print(\"\\nCreating submission...\")\n",
        "    sub_df = create_submission(preds, test_unique)\n",
        "    sub_df.to_csv(CFG.SUBMISSION_FILE, index=False)\n",
        "    print(f\"Saved: {CFG.SUBMISSION_FILE}\")\n",
        "    print(sub_df.head(10))\n",
        "    \n",
        "    # Cleanup\n",
        "    del models\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Done!\")\n",
        "    print(\"=\"*60)\n",
        "    return sub_df\n",
        "\n",
        "\n",
        "# Run\n",
        "submission = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
